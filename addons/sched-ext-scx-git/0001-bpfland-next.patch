From d2231b0aedd46422fd512f9f2ceb2705db219a7a Mon Sep 17 00:00:00 2001
From: Andrea Righi <andrea.righi@canonical.com>
Date: Wed, 3 Jul 2024 08:52:50 +0200
Subject: [PATCH 1/5] scx_bpfland: drop built-in idle CPU selection logic

Small refactoring of the idle CPU selection logic:
 - optimize idle CPU selection for tasks that can run on a single CPU
 - drop the built-in idle selection policy and completely rely on the
   custom one

Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
---
 scheds/rust/scx_bpfland/src/bpf/main.bpf.c | 122 +++++++++++----------
 scheds/rust/scx_bpfland/src/main.rs        |   5 -
 2 files changed, 63 insertions(+), 64 deletions(-)

diff --git a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
index 16f1764..2e2d6ed 100644
--- a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
@@ -50,11 +50,6 @@ const volatile u64 slice_ns_min = 500000;
  */
 const volatile u64 slice_ns_lag;
 
-/*
- * Enable built-in idle selection logic.
- */
-const volatile bool builtin_idle;
-
 /*
  * Threshold of voluntary context switches used to classify a task as
  * interactive.
@@ -166,6 +161,30 @@ static int calloc_cpumask(struct bpf_cpumask **p_cpumask)
 	return 0;
 }
 
+/*
+ * Set the state of a CPU in a cpumask.
+ */
+static bool set_cpu_state(struct bpf_cpumask *cpumask, s32 cpu, bool state)
+{
+	struct bpf_cpumask *mask;
+	int ret = false;
+
+	bpf_rcu_read_lock();
+
+	mask = cpumask;
+	if (!mask)
+		goto out_rcu;
+	if (state)
+		ret = bpf_cpumask_test_and_set_cpu(cpu, mask);
+	else
+		ret = bpf_cpumask_test_and_clear_cpu(cpu, mask);
+
+out_rcu:
+	bpf_rcu_read_unlock();
+
+	return ret;
+}
+
 /*
  * Exponential weighted moving average (EWMA).
  *
@@ -299,9 +318,9 @@ static void dispatch_task(struct task_struct *p, u64 enq_flags)
 	if (!tctx)
 		return;
 	/*
-	 * Always dispatch interactive tasks to the priority DSQ and regular
-	 * tasks to the shared DSQ.
-	 */
+	* Always dispatch interactive tasks to the priority DSQ and regular
+	* tasks to the shared DSQ.
+	*/
 	if (tctx->is_interactive) {
 		scx_bpf_dispatch_vtime(p, PRIO_DSQ, slice, vtime, enq_flags);
 		__sync_fetch_and_add(&nr_prio_dispatches, 1);
@@ -313,27 +332,18 @@ static void dispatch_task(struct task_struct *p, u64 enq_flags)
 
 /*
  * Find an idle CPU in the system.
+ *
+ * NOTE: the idle CPU selection doesn't need to be formally perfect, it is
+ * totally fine to accept racy conditions and potentially make mistakes, by
+ * picking CPUs that are not idle or even offline, the logic has been designed
+ * to handle these mistakes in favor of a more efficient response and a reduced
+ * scheduling overhead.
  */
 static s32 pick_idle_cpu(struct task_struct *p, s32 prev_cpu, u64 wake_flags)
 {
 	const struct cpumask *online_cpumask, *idle_smtmask, *idle_cpumask;
-	bool prev_in_cand;
 	s32 cpu;
 
-	if (builtin_idle) {
-		bool is_idle = false;
-
-		/*
-		 * Find an idle CPU using the sched_ext built-in idle selection
-		 * logic.
-		 */
-		cpu = scx_bpf_select_cpu_dfl(p, prev_cpu, wake_flags, &is_idle);
-		if (is_idle)
-			return cpu;
-
-		return -ENOENT;
-	}
-
 	/*
 	 * Acquire the CPU masks to determine the online and idle CPUs in the
 	 * system.
@@ -342,14 +352,28 @@ static s32 pick_idle_cpu(struct task_struct *p, s32 prev_cpu, u64 wake_flags)
 	idle_smtmask = scx_bpf_get_idle_smtmask();
 	idle_cpumask = scx_bpf_get_idle_cpumask();
 
-	prev_in_cand = bpf_cpumask_test_cpu(prev_cpu, p->cpus_ptr);
+	/*
+	 * For tasks that can run only on a single CPU, we can simply verify if
+	 * their only allowed CPU is idle.
+	 */
+	if (p->nr_cpus_allowed == 1) {
+		cpu = bpf_cpumask_first(p->cpus_ptr);
 
+		if (scx_bpf_test_and_clear_cpu_idle(cpu))
+			goto out_put_cpumask;
+		else
+			goto out_not_found;
+	}
+
+	/*
+	 * Find the best idle CPU, prioritizing full idle cores in SMT systems.
+	 */
 	if (smt_enabled) {
 		/*
 		 * If the task can still run on the previously used CPU and
 		 * it's a full-idle core, keep using it.
 		 */
-		if (prev_in_cand &&
+		if (bpf_cpumask_test_cpu(prev_cpu, p->cpus_ptr) &&
 		    bpf_cpumask_test_cpu(prev_cpu, idle_smtmask) &&
 		    scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
 			cpu = prev_cpu;
@@ -366,10 +390,11 @@ static s32 pick_idle_cpu(struct task_struct *p, s32 prev_cpu, u64 wake_flags)
 	}
 
 	/*
-	 * If a full-idle core can't be found (or if it's not an SMT system)
+	 * If a full-idle core can't be found (or if this is not an SMT system)
 	 * try to re-use the same CPU, even if it's not in a full-idle core.
 	 */
-	if (prev_in_cand && scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
+	if (bpf_cpumask_test_cpu(prev_cpu, p->cpus_ptr) &&
+	    scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
 		cpu = prev_cpu;
 		goto out_put_cpumask;
 	}
@@ -390,16 +415,16 @@ static s32 pick_idle_cpu(struct task_struct *p, s32 prev_cpu, u64 wake_flags)
 		struct task_ctx *tctx;
 
 		/*
-		 * If we are waking up a task and we couldn't find any idle CPU
-		 * to use, at least set the task as interactive, so that it can
-		 * be dispatched as soon as possible on the first CPU
-		 * available.
+		 * If we are waking up a task and we can't use the current CPU
+		 * at least set the task as interactive, so that it can be
+		 * dispatched as soon as possible on the first CPU available.
 		 */
 		tctx = lookup_task_ctx(p);
 		if (tctx)
 			tctx->is_interactive = true;
 	}
 
+out_not_found:
 	/*
 	 * If all the previous attempts have failed, dispatch the task to the
 	 * first CPU that will become available.
@@ -604,37 +629,16 @@ void BPF_STRUCT_OPS(bpfland_enable, struct task_struct *p)
 	p->scx.dsq_vtime = vtime_now;
 }
 
-/*
- * Set the offline state of a CPU, updating the global offline cpumask.
- */
-static void set_cpu_offline(s32 cpu, bool state)
-{
-	struct bpf_cpumask *offline;
-
-	bpf_rcu_read_lock();
-
-	offline = offline_cpumask;
-	if (!offline)
-		goto out_rcu;
-	if (state)
-		bpf_cpumask_set_cpu(cpu, offline);
-	else
-		bpf_cpumask_clear_cpu(cpu, offline);
-
-out_rcu:
-	bpf_rcu_read_unlock();
-}
-
 void BPF_STRUCT_OPS(bpfland_cpu_online, s32 cpu)
 {
 	/* Set the CPU state to offline */
-	set_cpu_offline(cpu, false);
+	set_cpu_state(offline_cpumask, cpu, false);
 }
 
 void BPF_STRUCT_OPS(bpfland_cpu_offline, s32 cpu)
 {
 	/* Set the CPU state to online */
-	set_cpu_offline(cpu, true);
+	set_cpu_state(offline_cpumask, cpu, true);
 }
 
 s32 BPF_STRUCT_OPS(bpfland_init_task, struct task_struct *p,
@@ -649,7 +653,7 @@ s32 BPF_STRUCT_OPS(bpfland_init_task, struct task_struct *p,
 
 s32 BPF_STRUCT_OPS_SLEEPABLE(bpfland_init)
 {
-	struct bpf_cpumask *offline;
+	struct bpf_cpumask *mask;
 	int err;
 	s32 cpu;
 
@@ -680,12 +684,12 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(bpfland_init)
 	/* Initialize the offline CPU mask */
 	bpf_rcu_read_lock();
 	err = calloc_cpumask(&offline_cpumask);
-	offline = offline_cpumask;
-	if (err || !offline)
-		err -ENOMEM;
+	mask = offline_cpumask;
+	if (!mask)
+		err = -ENOMEM;
 	bpf_rcu_read_unlock();
 
-	return 0;
+	return err;
 }
 
 void BPF_STRUCT_OPS(bpfland_exit, struct scx_exit_info *ei)
diff --git a/scheds/rust/scx_bpfland/src/main.rs b/scheds/rust/scx_bpfland/src/main.rs
index e8cf628..dd52598 100644
--- a/scheds/rust/scx_bpfland/src/main.rs
+++ b/scheds/rust/scx_bpfland/src/main.rs
@@ -78,10 +78,6 @@ struct Opts {
     #[clap(short = 'c', long, default_value = "10")]
     nvcsw_thresh: u64,
 
-    /// Enable direct dispatch via sched_ext built-in idle selection logic.
-    #[clap(short = 'i', long, action = clap::ArgAction::SetTrue)]
-    builtin_idle: bool,
-
     /// Enable the Prometheus endpoint for metrics on port 9000.
     #[clap(short = 'p', long, action = clap::ArgAction::SetTrue)]
     enable_prometheus: bool,
@@ -174,7 +170,6 @@ impl<'a> Scheduler<'a> {
         skel.rodata_mut().slice_ns_min = opts.slice_us_min * 1000;
         skel.rodata_mut().slice_ns_lag = opts.slice_us_lag * 1000;
         skel.rodata_mut().nvcsw_thresh = opts.nvcsw_thresh;
-        skel.rodata_mut().builtin_idle = opts.builtin_idle;
 
         // Attach the scheduler.
         let mut skel = scx_ops_load!(skel, bpfland_ops, uei)?;
-- 
2.45.2


From e8a4d350adff23508a99131f5472efe937df394d Mon Sep 17 00:00:00 2001
From: Andrea Righi <andrea.righi@canonical.com>
Date: Wed, 3 Jul 2024 09:02:03 +0200
Subject: [PATCH 2/5] scx_bpfland: unify dispatching kthreads with direct CPU
 dispatches

Always use direct CPU dispatch for kthreads, there is no need to treat
kthreads in a special way, simply reuse direct CPU dispatch to
prioritize them.

Moreover, change direct CPU dispatches to use scx_bpf_dispatch_vtime(),
since we may dispatch multiple tasks to the same per-CPU DSQ now.

Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
---
 scheds/rust/scx_bpfland/src/bpf/main.bpf.c | 91 ++++++++--------------
 scheds/rust/scx_bpfland/src/main.rs        | 13 +---
 2 files changed, 33 insertions(+), 71 deletions(-)

diff --git a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
index 2e2d6ed..87b3946 100644
--- a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
@@ -59,8 +59,7 @@ const volatile u64 nvcsw_thresh = 10;
 /*
  * Scheduling statistics.
  */
-volatile u64 nr_direct_dispatches, nr_kthread_dispatches,
-	     nr_shared_dispatches, nr_prio_dispatches;
+volatile u64 nr_direct_dispatches, nr_shared_dispatches, nr_prio_dispatches;
 
 /*
  * Amount of currently running tasks.
@@ -249,22 +248,6 @@ static u64 cpu_to_dsq(s32 cpu)
 	return (u64)cpu;
 }
 
-/*
- * Dispatch a per-CPU kthread directly to the local CPU DSQ.
- */
-static void dispatch_kthread(struct task_struct *p, u64 enq_flags)
-{
-	u64 slice = task_slice(p);
-
-	/*
-	 * Use the local CPU DSQ directly for per-CPU kthreads, to give them
-	 * maximum priority.
-	 */
-	scx_bpf_dispatch(p, SCX_DSQ_LOCAL, slice, enq_flags);
-
-	__sync_fetch_and_add(&nr_kthread_dispatches, 1);
-}
-
 /*
  * Dispatch a task directly to the assigned CPU DSQ (used when an idle CPU is
  * found).
@@ -272,6 +255,7 @@ static void dispatch_kthread(struct task_struct *p, u64 enq_flags)
 static int dispatch_direct_cpu(struct task_struct *p, s32 cpu)
 {
 	u64 slice = task_slice(p);
+	u64 vtime = task_vtime(p);
 	u64 dsq_id = cpu_to_dsq(cpu);
 
 	/*
@@ -281,14 +265,7 @@ static int dispatch_direct_cpu(struct task_struct *p, s32 cpu)
 	if (!bpf_cpumask_test_cpu(cpu, p->cpus_ptr))
 		return -EINVAL;
 
-	/*
-	 * We don't need to use vtime here, because we basically dispatch one
-	 * task at a time when the corresponding CPU is idle.
-	 *
-	 * We could also use SCX_DSQ_LOCAL, but we want to distinguish regular
-	 * tasks from per-CPU kthreads to give more priority to the latter.
-	 */
-	scx_bpf_dispatch(p, dsq_id, slice, 0);
+	scx_bpf_dispatch_vtime(p, dsq_id, slice, vtime, 0);
 	__sync_fetch_and_add(&nr_direct_dispatches, 1);
 
 	/*
@@ -305,31 +282,6 @@ static int dispatch_direct_cpu(struct task_struct *p, s32 cpu)
 	return 0;
 }
 
-/*
- * Dispatch a regular task.
- */
-static void dispatch_task(struct task_struct *p, u64 enq_flags)
-{
-	u64 vtime = task_vtime(p);
-	u64 slice = task_slice(p);
-	struct task_ctx *tctx;
-
-	tctx = lookup_task_ctx(p);
-	if (!tctx)
-		return;
-	/*
-	* Always dispatch interactive tasks to the priority DSQ and regular
-	* tasks to the shared DSQ.
-	*/
-	if (tctx->is_interactive) {
-		scx_bpf_dispatch_vtime(p, PRIO_DSQ, slice, vtime, enq_flags);
-		__sync_fetch_and_add(&nr_prio_dispatches, 1);
-	} else {
-		scx_bpf_dispatch_vtime(p, SHARED_DSQ, slice, vtime, enq_flags);
-		__sync_fetch_and_add(&nr_shared_dispatches, 1);
-	}
-}
-
 /*
  * Find an idle CPU in the system.
  *
@@ -450,10 +402,18 @@ s32 BPF_STRUCT_OPS(bpfland_select_cpu, struct task_struct *p, s32 prev_cpu, u64
 	return prev_cpu;
 }
 
+/*
+ * Dispatch all the other tasks that were not dispatched directly in
+ * select_cpu().
+ */
 void BPF_STRUCT_OPS(bpfland_enqueue, struct task_struct *p, u64 enq_flags)
 {
+	u64 vtime = task_vtime(p);
+	u64 slice = task_slice(p);
+	struct task_ctx *tctx;
+
 	/*
-	 * Always dispatch per-CPU kthreads immediately.
+	 * Always dispatch per-CPU kthreads directly on their target CPU.
 	 *
 	 * This allows to prioritize critical kernel threads that may
 	 * potentially slow down the entire system if they are blocked for too
@@ -465,15 +425,26 @@ void BPF_STRUCT_OPS(bpfland_enqueue, struct task_struct *p, u64 enq_flags)
 	 * this scheduler is desktop usage, this shouldn't be a problem.
 	 */
 	if (is_kthread(p) && p->nr_cpus_allowed == 1) {
-		dispatch_kthread(p, enq_flags);
+		s32 cpu = scx_bpf_task_cpu(p);
+		dispatch_direct_cpu(p, cpu);
 		return;
 	}
 
+	tctx = lookup_task_ctx(p);
+	if (!tctx)
+		return;
+
 	/*
-	 * Dispatch all the other tasks that were not dispatched directly in
-	 * select_cpu().
+	 * Always dispatch interactive tasks to the priority DSQ and regular
+	 * tasks to the shared DSQ.
 	 */
-	dispatch_task(p, enq_flags);
+	if (tctx->is_interactive) {
+		scx_bpf_dispatch_vtime(p, PRIO_DSQ, slice, vtime, enq_flags);
+		__sync_fetch_and_add(&nr_prio_dispatches, 1);
+	} else {
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ, slice, vtime, enq_flags);
+		__sync_fetch_and_add(&nr_shared_dispatches, 1);
+	}
 }
 
 /*
@@ -482,7 +453,7 @@ void BPF_STRUCT_OPS(bpfland_enqueue, struct task_struct *p, u64 enq_flags)
  * These tasks will be consumed on other active CPUs to prevent indefinite
  * stalling.
  */
-static int dispatch_offline_cpus(s32 cpu)
+static int consume_offline_cpus(s32 cpu)
 {
 	u64 cpu_max = scx_bpf_nr_cpu_ids();
 	struct bpf_cpumask *offline;
@@ -521,8 +492,8 @@ out_rcu:
 void BPF_STRUCT_OPS(bpfland_dispatch, s32 cpu, struct task_struct *prev)
 {
 	/*
-	 * First consume directly dispatched tasks, so that they can
-	 * immediately use the CPU assigned in select_cpu().
+	 * Consume directly dispatched tasks, so that they can immediately use
+	 * the CPU assigned in select_cpu().
 	 */
 	if (scx_bpf_consume(cpu_to_dsq(cpu)))
 		return;
@@ -531,7 +502,7 @@ void BPF_STRUCT_OPS(bpfland_dispatch, s32 cpu, struct task_struct *prev)
 	 * Try also to steal tasks directly dispatched to CPUs that have gone
 	 * offline (this allows to prevent indefinite task stalls).
 	 */
-	if (!dispatch_offline_cpus(cpu))
+	if (!consume_offline_cpus(cpu))
 		return;
 
 	/*
diff --git a/scheds/rust/scx_bpfland/src/main.rs b/scheds/rust/scx_bpfland/src/main.rs
index dd52598..3c2e248 100644
--- a/scheds/rust/scx_bpfland/src/main.rs
+++ b/scheds/rust/scx_bpfland/src/main.rs
@@ -97,7 +97,6 @@ struct Opts {
 
 struct Metrics {
     nr_running: Gauge,
-    nr_kthread_dispatches: Gauge,
     nr_direct_dispatches: Gauge,
     nr_prio_dispatches: Gauge,
     nr_shared_dispatches: Gauge,
@@ -109,9 +108,6 @@ impl Metrics {
             nr_running: gauge!(
                 "nr_running", "info" => "Number of running tasks"
             ),
-            nr_kthread_dispatches: gauge!(
-                "nr_kthread_dispatches", "info" => "Number of kthread dispatches"
-            ),
             nr_direct_dispatches: gauge!(
                 "nr_direct_dispatches", "info" => "Number of direct dispatches"
             ),
@@ -191,18 +187,14 @@ impl<'a> Scheduler<'a> {
     }
 
     fn update_stats(&mut self) {
-        let nr_running = self.skel.bss().nr_running;
         let nr_cpus = libbpf_rs::num_possible_cpus().unwrap();
-        let nr_kthread_dispatches = self.skel.bss().nr_kthread_dispatches;
+        let nr_running = self.skel.bss().nr_running;
         let nr_direct_dispatches = self.skel.bss().nr_direct_dispatches;
         let nr_prio_dispatches = self.skel.bss().nr_prio_dispatches;
         let nr_shared_dispatches = self.skel.bss().nr_shared_dispatches;
 
         // Update Prometheus statistics.
         self.metrics.nr_running.set(nr_running as f64);
-        self.metrics
-            .nr_kthread_dispatches
-            .set(nr_kthread_dispatches as f64);
         self.metrics
             .nr_direct_dispatches
             .set(nr_direct_dispatches as f64);
@@ -214,10 +206,9 @@ impl<'a> Scheduler<'a> {
             .set(nr_shared_dispatches as f64);
 
         // Log scheduling statistics.
-        info!("running={}/{} nr_kthread_dispatches={} nr_direct_dispatches={} nr_prio_dispatches={} nr_shared_dispatches={}",
+        info!("running={}/{} direct_dispatches={} prio_dispatches={} shared_dispatches={}",
             nr_running,
             nr_cpus,
-            nr_kthread_dispatches,
             nr_direct_dispatches,
             nr_prio_dispatches,
             nr_shared_dispatches);
-- 
2.45.2


From 27176fb6ed204e58724e18b1a3e6186a5113b1ae Mon Sep 17 00:00:00 2001
From: Andrea Righi <andrea.righi@canonical.com>
Date: Tue, 2 Jul 2024 19:17:11 +0200
Subject: [PATCH 3/5] scx_bpfland: prevent starvation of regular tasks

Introduce a threshold to prevent starvation of regular tasks over
interactive tasks.

The threshold will ensure that at least one regular task is scheduled
after too many consecutive interactive tasks are scheduled.

This threshold can be tuned using the new --starvation-thresh command
line parameter.

Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
---
 scheds/rust/scx_bpfland/src/bpf/main.bpf.c | 51 ++++++++++++++++++----
 scheds/rust/scx_bpfland/src/main.rs        |  6 +++
 2 files changed, 48 insertions(+), 9 deletions(-)

diff --git a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
index 87b3946..6fc44eb 100644
--- a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
@@ -56,6 +56,13 @@ const volatile u64 slice_ns_lag;
  */
 const volatile u64 nvcsw_thresh = 10;
 
+/*
+ * Schedule up to the starvation_thresh number of priority tasks before being
+ * required to schedule a regular task. This ensures that regular tasks are not
+ * starved by interactive tasks.
+ */
+const volatile u64 starvation_thresh = 100;
+
 /*
  * Scheduling statistics.
  */
@@ -489,8 +496,43 @@ out_rcu:
 	return ret;
 }
 
+/*
+ * Check if consecutive scheduling of interactive tasks is starving regular
+ * tasks.
+ *
+ * Return true if tasks in the shared DSQ are starving, false otherwise.
+ */
+static bool is_shared_starving(void)
+{
+	static int starvation_cnt;
+
+	if (!starvation_thresh)
+		return false;
+	/*
+	 * NOTE: it is totally fine for this code to be racy, the threshold
+	 * doesn't need to be perfectly enforced.
+	 */
+	if (starvation_cnt++ < starvation_thresh)
+		return false;
+	starvation_cnt = 0;
+
+	return true;
+}
+
 void BPF_STRUCT_OPS(bpfland_dispatch, s32 cpu, struct task_struct *prev)
 {
+	/*
+	 * Make sure we are not staving tasks from the shared DSQ.
+	 *
+	 * In order to limit potential starvation conditions the scheduler uses
+	 * a threshold to ensure that at least one task from the shared DSQ is
+	 * consumed after too many consecutive interactive tasks are consumed
+	 * from the priority DSQ.
+	 */
+	if (is_shared_starving())
+		if (scx_bpf_consume(SHARED_DSQ))
+			return;
+
 	/*
 	 * Consume directly dispatched tasks, so that they can immediately use
 	 * the CPU assigned in select_cpu().
@@ -507,15 +549,6 @@ void BPF_STRUCT_OPS(bpfland_dispatch, s32 cpu, struct task_struct *prev)
 
 	/*
 	 * Then always consume interactive tasks before regular tasks.
-	 *
-	 * This is fine and we shouldn't have starvation, because interactive
-	 * tasks are classified by their amount of voluntary context switches,
-	 * so they should naturally release the CPU quickly and give a chance
-	 * to the regular tasks to run.
-	 *
-	 * TODO: Add a tunable setting to limit the number of priority tasks
-	 * dispatched. Once this limit is reached, at least one regular task
-	 * must be dispatched.
 	 */
 	if (scx_bpf_consume(PRIO_DSQ))
 		return;
diff --git a/scheds/rust/scx_bpfland/src/main.rs b/scheds/rust/scx_bpfland/src/main.rs
index 3c2e248..d984fb0 100644
--- a/scheds/rust/scx_bpfland/src/main.rs
+++ b/scheds/rust/scx_bpfland/src/main.rs
@@ -78,6 +78,11 @@ struct Opts {
     #[clap(short = 'c', long, default_value = "10")]
     nvcsw_thresh: u64,
 
+    /// Prevent the starvation of regular tasks by limiting the number of interactive tasks that
+    /// can be scheduled consecutively (0 = disable starvation prevention).
+    #[clap(short = 't', long, default_value = "100")]
+    starvation_thresh: u64,
+
     /// Enable the Prometheus endpoint for metrics on port 9000.
     #[clap(short = 'p', long, action = clap::ArgAction::SetTrue)]
     enable_prometheus: bool,
@@ -166,6 +171,7 @@ impl<'a> Scheduler<'a> {
         skel.rodata_mut().slice_ns_min = opts.slice_us_min * 1000;
         skel.rodata_mut().slice_ns_lag = opts.slice_us_lag * 1000;
         skel.rodata_mut().nvcsw_thresh = opts.nvcsw_thresh;
+        skel.rodata_mut().starvation_thresh = opts.starvation_thresh;
 
         // Attach the scheduler.
         let mut skel = scx_ops_load!(skel, bpfland_ops, uei)?;
-- 
2.45.2


From e4ce742b690e4bcf929c651d29e98460f66f0596 Mon Sep 17 00:00:00 2001
From: Andrea Righi <andrea.righi@canonical.com>
Date: Wed, 3 Jul 2024 09:30:06 +0200
Subject: [PATCH 4/5] scx_bpfland: ensure task time slice never exceeds the
 slice_ns limit

Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
---
 scheds/rust/scx_bpfland/src/bpf/main.bpf.c | 11 ++++++++---
 1 file changed, 8 insertions(+), 3 deletions(-)

diff --git a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
index 6fc44eb..92f9f93 100644
--- a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
@@ -237,9 +237,7 @@ static inline u64 task_vtime(struct task_struct *p)
  */
 static inline u64 task_slice(struct task_struct *p)
 {
-	u64 slice = p->scx.slice;
-
-	return MAX(slice, slice_ns_min);
+	return MAX(p->scx.slice, slice_ns_min);
 }
 
 /*
@@ -565,6 +563,13 @@ void BPF_STRUCT_OPS(bpfland_running, struct task_struct *p)
 	if (vtime_before(vtime_now, p->scx.dsq_vtime))
 		vtime_now = p->scx.dsq_vtime;
 
+	/*
+	 * Ensure time slice never exceeds slice_ns when a task is started on a
+	 * CPU.
+	 */
+	if (p->scx.slice > slice_ns)
+		p->scx.slice = slice_ns;
+
 	__sync_fetch_and_add(&nr_running, 1);
 }
 
-- 
2.45.2


From 3bac5e442944d6f3dcabf31147b25ad8f902a09b Mon Sep 17 00:00:00 2001
From: Andrea Righi <andrea.righi@canonical.com>
Date: Tue, 2 Jul 2024 08:52:44 +0200
Subject: [PATCH 5/5] scx_bpfland: adjust default time slice to 5ms

Reduce the default time slice down to 5ms for a faster reaction and
system responsiveness when the system is overcomissioned.

This also helps to provide a more predictable level of performance.

Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
---
 scheds/rust/scx_bpfland/src/bpf/main.bpf.c | 2 +-
 scheds/rust/scx_bpfland/src/main.rs        | 2 +-
 2 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
index 92f9f93..a114b79 100644
--- a/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_bpfland/src/bpf/main.bpf.c
@@ -34,7 +34,7 @@ const volatile bool debug;
 /*
  * Default task time slice.
  */
-const volatile u64 slice_ns = SCX_SLICE_DFL;
+const volatile u64 slice_ns = 5000000;
 
 /*
  * Time slice used when system is over commissioned.
diff --git a/scheds/rust/scx_bpfland/src/main.rs b/scheds/rust/scx_bpfland/src/main.rs
index d984fb0..c0ed47a 100644
--- a/scheds/rust/scx_bpfland/src/main.rs
+++ b/scheds/rust/scx_bpfland/src/main.rs
@@ -59,7 +59,7 @@ struct Opts {
     exit_dump_len: u32,
 
     /// Maximum scheduling slice duration in microseconds.
-    #[clap(short = 's', long, default_value = "20000")]
+    #[clap(short = 's', long, default_value = "5000")]
     slice_us: u64,
 
     /// Minimum scheduling slice duration in microseconds.
-- 
2.45.2

