From eff444516fd95c30c9b5c7cec2de58315b58166d Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Tue, 16 Jul 2024 23:48:26 +0900
Subject: [PATCH 01/20] scx_lavd: directly measure service time for eligibility
 enforcement

Estimating the service time from run time and frequency is not
incorrect. However, it reacts slowly to sudden changes since it relies
on the moving average. Hence, we directly measure the service time to
enforce fairness.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h     |  3 ++
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 70 ++++++++++++++++++++-----
 2 files changed, 61 insertions(+), 12 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index a0d03d1..ecfdd60 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -111,6 +111,7 @@ struct sys_stat {
 
 	volatile u64	load_ideal;	/* average ideal load of runnable tasks */
 	volatile u64	load_actual;	/* average actual load of runnable tasks */
+	volatile u64	avg_svc_time;	/* average service time per task */
 
 	volatile u32	avg_lat_cri;	/* average latency criticality (LC) */
 	volatile u32	max_lat_cri;	/* maximum latency criticality (LC) */
@@ -143,6 +144,7 @@ struct cpu_ctx {
 	volatile u64	load_ideal;	/* ideal loaf of runnable tasks */
 	volatile u64	load_actual;	/* actual load of runnable tasks */
 	volatile u64	load_run_time_ns; /* total runtime of runnable tasks */
+	volatile u64	tot_svc_time;	/* total service time on a CPU */
 	volatile u64	last_kick_clk;	/* when the CPU was kicked */
 
 	/*
@@ -206,6 +208,7 @@ struct task_ctx {
 
 	u64	wake_freq;		/* waking-up frequency in a second */
 	u64	load_actual;		/* task load derived from run_time and run_freq */
+	u64	svc_time;		/* total CPU time consumed for this task */
 
 	/*
 	 * Task deadline and time slice
diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index 2dd2042..18ce9ea 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -216,7 +216,12 @@ const volatile u16 cpu_order[LAVD_CPU_ID_MAX]; /* ordered by cpus->core->llc->nu
 /*
  * Logical current clock
  */
-u64			cur_logical_clk;
+static u64		cur_logical_clk;
+
+/*
+ * Current service time
+ */
+static u64		cur_svc_time;
 
 /*
  * Options
@@ -639,6 +644,7 @@ struct sys_stat_ctx {
 	u64		compute_total;
 	u64		load_actual;
 	u64		load_ideal;
+	u64		tot_svc_time;
 	u64		load_run_time_ns;
 	s32		max_lat_cri;
 	s32		min_lat_cri;
@@ -680,6 +686,7 @@ static void collect_sys_stat(struct sys_stat_ctx *c)
 		c->load_ideal += cpuc->load_ideal;
 		c->load_actual += cpuc->load_actual;
 		c->load_run_time_ns += cpuc->load_run_time_ns;
+		c->tot_svc_time += cpuc->tot_svc_time;
 
 		/*
 		 * Accumulate task's latency criticlity information.
@@ -806,6 +813,9 @@ static void update_sys_stat_next(struct sys_stat_ctx *c)
 
 	stat_next->nr_violation =
 		calc_avg32(stat_cur->nr_violation, c->nr_violation);
+
+	stat_next->avg_svc_time = (c->sched_nr == 0) ? 0 :
+				  c->tot_svc_time / c->sched_nr;
 }
 
 static void calc_inc1k(struct sys_stat_ctx *c)
@@ -1012,15 +1022,10 @@ static u32 calc_greedy_ratio(struct task_struct *p, struct task_ctx *taskc)
 	/*
 	 * The greedy ratio of a task represents how much time the task
 	 * overspent CPU time compared to the ideal, fair CPU allocation. It is
-	 * the ratio of task's actual ratio to its ideal ratio. The actual
-	 * ratio is the ratio of the task's average runtime to the total
-	 * runtime in a system. The ideal ratio is the ratio of the task's
-	 * weight, derived from its nice priority, to the sum of weights in a
-	 * system. We use the moving averages (EWMA: exponentially weighted
-	 * moving average) instead of the actual summation, which never decays.
-	 */
-	ratio = (1000 * taskc->load_actual * stat_cur->load_ideal) /
-		(stat_cur->load_actual * get_task_load_ideal(p));
+	 * the ratio of task's actual service time to average service time in a
+	 * system.
+	 */
+	ratio = (1000 * taskc->svc_time) / stat_cur->avg_svc_time;
 	taskc->greedy_ratio = ratio;
 	return ratio;
 }
@@ -1316,6 +1321,7 @@ static u64 calc_virtual_deadline_delta(struct task_struct *p,
 	is_wakeup = is_wakeup_ef(enq_flags);
 	weight = calc_latency_weight(p, taskc, cpuc, is_wakeup);
 	vdeadline_delta_ns = (((taskc->run_time_ns + 1) * weight) + 1000) / 1000;
+
 	/*
 	 * When a system is overloaded (>1000), stretch time space so make time
 	 * tick logically slower to give room to execute the overloaded tasks.
@@ -1465,6 +1471,12 @@ static void update_stat_for_running(struct task_struct *p,
 	 */
 	WRITE_ONCE(cur_logical_clk, taskc->vdeadline_log_clk);
 
+	/*
+	 * Update the current service time if necessary.
+	 */
+	if (cur_svc_time < taskc->svc_time)
+		WRITE_ONCE(cur_svc_time, taskc->svc_time);
+
 	/*
 	 * Since this is the start of a new schedule for @p, we update run
 	 * frequency in a second using an exponential weighted moving average.
@@ -1521,12 +1533,20 @@ static void update_stat_for_running(struct task_struct *p,
 	taskc->last_running_clk = now;
 }
 
+static u64 calc_svc_time(struct task_struct *p, struct task_ctx *taskc)
+{
+	/*
+	 * Scale the execution time by the inverse of the weight and charge.
+	 */
+	return (taskc->last_stopping_clk - taskc->last_running_clk) / p->scx.weight;
+}
+
 static void update_stat_for_stopping(struct task_struct *p,
 				     struct task_ctx *taskc,
 				     struct cpu_ctx *cpuc)
 {
 	u64 now = bpf_ktime_get_ns();
-	u64 old_run_time_ns, suspended_duration;
+	u64 old_run_time_ns, suspended_duration, task_svc_time;
 
 	/*
 	 * Update task's run_time. When a task is scheduled consecutively
@@ -1544,6 +1564,8 @@ static void update_stat_for_stopping(struct task_struct *p,
 	taskc->run_time_ns = calc_avg(taskc->run_time_ns,
 				      taskc->acc_run_time_ns);
 	taskc->last_stopping_clk = now;
+	task_svc_time = calc_svc_time(p, taskc);
+	taskc->svc_time += task_svc_time;
 	taskc->victim_cpu = (s32)LAVD_CPU_ID_NONE;
 
 	/*
@@ -1553,6 +1575,11 @@ static void update_stat_for_stopping(struct task_struct *p,
 	cpuc->load_run_time_ns = cpuc->load_run_time_ns -
 				 clamp_time_slice_ns(old_run_time_ns) +
 				 clamp_time_slice_ns(taskc->run_time_ns);
+
+	/*
+	 * Increase total service time of this CPU.
+	 */
+	cpuc->tot_svc_time += task_svc_time;
 }
 
 static void update_stat_for_quiescent(struct task_struct *p,
@@ -2782,6 +2809,22 @@ static void init_task_ctx(struct task_struct *p, struct task_ctx *taskc)
 	taskc->slice_ns = 0;
 }
 
+void BPF_STRUCT_OPS(lavd_enable, struct task_struct *p)
+{
+	struct task_ctx *taskc;
+
+	/*
+	 * Set task's service time to the current, minimum service time.
+	 */
+	taskc = get_task_ctx(p);
+	if (!taskc) {
+		scx_bpf_error("task_ctx_stor first lookup failed");
+		return;
+	}
+
+	taskc->svc_time = READ_ONCE(cur_svc_time);
+}
+
 s32 BPF_STRUCT_OPS(lavd_init_task, struct task_struct *p,
 		   struct scx_init_task_args *args)
 {
@@ -2957,9 +3000,11 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_init)
 		return err;
 
 	/*
-	 * Initilize the current logical clock.
+	 * Initilize the current logical clock and service time.
 	 */
 	WRITE_ONCE(cur_logical_clk, 0);
+	WRITE_ONCE(cur_svc_time, 0);
+
 	return err;
 }
 
@@ -2980,6 +3025,7 @@ SCX_OPS_DEFINE(lavd_ops,
 	       .cpu_online		= (void *)lavd_cpu_online,
 	       .cpu_offline		= (void *)lavd_cpu_offline,
 	       .update_idle		= (void *)lavd_update_idle,
+	       .enable			= (void *)lavd_enable,
 	       .init_task		= (void *)lavd_init_task,
 	       .init			= (void *)lavd_init,
 	       .exit			= (void *)lavd_exit,
-- 
2.45.2


From adfbf3934c2e568f179d4c1b072505609e023e3a Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Tue, 16 Jul 2024 23:52:23 +0900
Subject: [PATCH 02/20] scx_lavd: tuning the max ineligible duration

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index ecfdd60..b292993 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -78,7 +78,7 @@ enum consts {
 	LAVD_LAT_WEIGHT_FT		= 88761,
 
 	LAVD_ELIGIBLE_TIME_LAT_FT	= 16,
-	LAVD_ELIGIBLE_TIME_MAX		= (100 * NSEC_PER_USEC),
+	LAVD_ELIGIBLE_TIME_MAX		= (10 * LAVD_TARGETED_LATENCY_NS),
 
 	LAVD_CPU_UTIL_MAX		= 1000, /* 100.0% */
 	LAVD_CPU_UTIL_MAX_FOR_CPUPERF	= 850, /* 85.0% */
-- 
2.45.2


From 971bb2e024f18ad4d045c49a10005bb0afe27139 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Tue, 16 Jul 2024 23:54:15 +0900
Subject: [PATCH 03/20] scx_lavd: pretty formatting for ineligible duration

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/main.rs | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/main.rs b/scheds/rust/scx_lavd/src/main.rs
index 43c1bc9..4e3ead5 100644
--- a/scheds/rust/scx_lavd/src/main.rs
+++ b/scheds/rust/scx_lavd/src/main.rs
@@ -191,7 +191,7 @@ impl<'a> Scheduler<'a> {
             info!(
                 "| {:6} | {:7} | {:17} \
                    | {:4} | {:4} | {:9} \
-                   | {:6} | {:8} | {:7} \
+                   | {:14} | {:8} | {:7} \
                    | {:8} | {:4} | {:7} \
                    | {:8} | {:7} | {:9} \
                    | {:9} | {:9} | {:9} \
@@ -231,7 +231,7 @@ impl<'a> Scheduler<'a> {
         info!(
             "| {:6} | {:7} | {:17} \
                | {:4} | {:4} | {:9} \
-               | {:6} | {:8} | {:7} \
+               | {:14} | {:8} | {:7} \
                | {:8} | {:4} | {:7} \
                | {:8} | {:7} | {:9} \
                | {:9} | {:9} | {:9} \
-- 
2.45.2


From c84b73e9717269a304baf8215e5f31c4040161ad Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Wed, 17 Jul 2024 10:34:34 +0900
Subject: [PATCH 04/20] scx_lavd: rename LAVD_GLOBAL_DSQ to LAVD_ELIGIBLE_DSQ

This is a prep to add a global ineligible dsq.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h     |  2 +-
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 20 ++++++++++----------
 2 files changed, 11 insertions(+), 11 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index b292993..2f9de16 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -98,7 +98,7 @@ enum consts {
 	LAVD_TC_CPU_PIN_INTERVAL_DIV	= (LAVD_TC_CPU_PIN_INTERVAL /
 					   LAVD_SYS_STAT_INTERVAL_NS),
 
-	LAVD_GLOBAL_DSQ			= 0,
+	LAVD_ELIGIBLE_DSQ		= 0, /* a global DSQ for eligible tasks */
 };
 
 /*
diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index 18ce9ea..e6009b7 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -1390,7 +1390,7 @@ static u64 calc_time_slice(struct task_struct *p, struct task_ctx *taskc)
 	 * The time slice should be short enough to schedule all runnable tasks
 	 * at least once within a targeted latency.
 	 */
-	nr_queued = (u64)scx_bpf_dsq_nr_queued(LAVD_GLOBAL_DSQ) + 1;
+	nr_queued = (u64)scx_bpf_dsq_nr_queued(LAVD_ELIGIBLE_DSQ) + 1;
 	slice = (LAVD_TARGETED_LATENCY_NS * stat_cur->nr_active) / nr_queued;
 	if (stat_cur->load_factor < 1000 && is_eligible(taskc)) {
 		slice += (LAVD_SLICE_BOOST_MAX_FT * slice *
@@ -1907,7 +1907,7 @@ static bool try_yield_current_cpu(struct task_struct *p_run,
 	prm_run.lat_prio = taskc_run->lat_prio;
 
 	bpf_rcu_read_lock();
-	bpf_for_each(scx_dsq, p_wait, LAVD_GLOBAL_DSQ, 0) {
+	bpf_for_each(scx_dsq, p_wait, LAVD_ELIGIBLE_DSQ, 0) {
 		taskc_wait = get_task_ctx(p_wait);
 		if (!taskc_wait)
 			break;
@@ -1932,7 +1932,7 @@ static bool try_yield_current_cpu(struct task_struct *p_run,
 		}
 
 		/*
-		 * Test only the first entry on the LAVD_GLOBAL_DSQ.
+		 * Test only the first entry on the LAVD_ELIGIBLE_DSQ.
 		 */
 		break;
 	}
@@ -1973,7 +1973,7 @@ static void put_global_rq(struct task_struct *p, struct task_ctx *taskc,
 	 * Enqueue the task to the global runqueue based on its virtual
 	 * deadline.
 	 */
-	scx_bpf_dispatch_vtime(p, LAVD_GLOBAL_DSQ, LAVD_SLICE_UNDECIDED,
+	scx_bpf_dispatch_vtime(p, LAVD_ELIGIBLE_DSQ, LAVD_SLICE_UNDECIDED,
 			       taskc->vdeadline_log_clk, enq_flags);
 
 }
@@ -2254,7 +2254,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 	 * If all CPUs are using, directly consume without checking CPU masks.
 	 */
 	if (use_full_cpus()) {
-		scx_bpf_consume(LAVD_GLOBAL_DSQ);
+		scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
 		return;
 	}
 
@@ -2275,7 +2275,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 	 */
 	if (bpf_cpumask_test_cpu(cpu, cast_mask(active)) ||
 	    bpf_cpumask_test_cpu(cpu, cast_mask(ovrflw))) {
-		scx_bpf_consume(LAVD_GLOBAL_DSQ);
+		scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
 		goto unlock_out;
 	}
 
@@ -2283,14 +2283,14 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 	 * If this CPU is not either in active or overflow CPUs, it tries to
 	 * find and run a task pinned to run on this CPU.
 	 */
-	bpf_for_each(scx_dsq, p, LAVD_GLOBAL_DSQ, 0) {
+	bpf_for_each(scx_dsq, p, LAVD_ELIGIBLE_DSQ, 0) {
 		/*
 		 * Prioritize kernel tasks because most kernel tasks are pinned
 		 * to a particular CPU and latency-critical (e.g., ksoftirqd,
 		 * kworker, etc).
 		 */
 		if (is_kernel_task(p)) {
-			scx_bpf_consume(LAVD_GLOBAL_DSQ);
+			scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
 			break;
 		}
 
@@ -2322,7 +2322,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 		 * cores. We will optimize this path after introducing per-core
 		 * DSQ.
 		 */
-		scx_bpf_consume(LAVD_GLOBAL_DSQ);
+		scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
 
 		/*
 		 * This is the first time a particular pinned user-space task
@@ -2968,7 +2968,7 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_init)
 	/*
 	 * Create a central task queue.
 	 */
-	err = scx_bpf_create_dsq(LAVD_GLOBAL_DSQ, -1);
+	err = scx_bpf_create_dsq(LAVD_ELIGIBLE_DSQ, -1);
 	if (err) {
 		scx_bpf_error("Failed to create a shared DSQ");
 		return err;
-- 
2.45.2


From 55e19ea5df3c627f22c3071e0bbc3c19be0ac751 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Wed, 17 Jul 2024 11:16:02 +0900
Subject: [PATCH 05/20] scx_lavd: do not prioritize a wake-up task in
 ops.select_cpu()

This is a prep for adding an ineligible DSQ.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 99 +------------------------
 1 file changed, 2 insertions(+), 97 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index e6009b7..a62bae2 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -1978,45 +1978,6 @@ static void put_global_rq(struct task_struct *p, struct task_ctx *taskc,
 
 }
 
-static bool prep_put_local_rq(struct task_struct *p, struct task_ctx *taskc,
-			      u64 enq_flags)
-{
-	struct cpu_ctx *cpuc;
-
-	cpuc = get_cpu_ctx();
-	if (!cpuc)
-		return false;
-
-	/*
-	 * Calculate when a tack can be scheduled. If a task is cannot be
-	 * scheduled soonish (i.e., the task is ineligible since
-	 * overscheduled), we do not put this to local run queue, which is for
-	 * immediate execution.
-	 *
-	 * Note that the task's time slice will be calculated and reassigned
-	 * right before running at ops.running().
-	 */
-	calc_when_to_run(p, taskc, cpuc, enq_flags);
-	if (!is_eligible(taskc))
-		return false;
-
-	return true;
-}
-
-static void put_local_rq_no_fail(struct task_struct *p, struct task_ctx *taskc,
-				 u64 enq_flags)
-{
-	/*
-	 * This task should be scheduled as soon as possible (e.g., wakened up)
-	 * so the deadline is no use and enqueued into a local DSQ, which
-	 * always follows a FIFO order.
-	 */
-	taskc->vdeadline_delta_ns = 0;
-	taskc->eligible_delta_ns = 0;
-	taskc->victim_cpu = (s32)LAVD_CPU_ID_NONE;
-	scx_bpf_dispatch(p, SCX_DSQ_LOCAL, LAVD_SLICE_UNDECIDED, enq_flags);
-}
-
 static bool could_run_on_prev(struct task_struct *p, s32 prev_cpu,
 			      struct bpf_cpumask *a_cpumask,
 			      struct bpf_cpumask *o_cpumask)
@@ -2143,68 +2104,12 @@ s32 BPF_STRUCT_OPS(lavd_select_cpu, struct task_struct *p, s32 prev_cpu,
 	
 	taskc = get_task_ctx(p);
 	if (!taskc)
-		goto try_yield_out;
-
-	/*
-	 * When a task wakes up, we should decide where to place the task at
-	 * ops.select_cpu(). We make a best effort to find an idle CPU. If
-	 * there is an idle CPU and a task is a true-wake-up task (not just
-	 * fork-ed or execv-ed), we consider such a task as a latency-critical
-	 * task, so directly dispatch to the local FIFO queue of the chosen
-	 * CPU. If the task is directly dispatched here, the sched_ext won't
-	 * call ops.enqueue().
-	 */
-	if (!is_wakeup_wf(wake_flags)) {
-		cpu_id = pick_cpu(p, taskc, prev_cpu, wake_flags, &found_idle);
-		if (found_idle) {
-			put_local_rq_no_fail(p, taskc, 0);
-			return cpu_id;
-		}
-
-		goto try_yield_out;
-	}
+		return prev_cpu;
 
-	/*
-	 * Prepare to put a task into a local queue. If the task is
-	 * over-scheduled or any error happens during the preparation, it won't
-	 * be put into the local queue. Instead, the task will be put into the
-	 * global queue during ops.enqueue().
-	 */
-	if (!prep_put_local_rq(p, taskc, 0))
-		goto try_yield_out;
-
-	/*
-	 * If the task can be put into the local queue, find an idle CPU first.
-	 * If there is an idle CPU, put the task into the local queue as
-	 * planned. Otherwise, let ops.enqueue() put the task into the global
-	 * queue.
-	 *
-	 * Note that once an idle CPU is successfully picked (i.e., found_idle
-	 * == true), then the picked CPU must be returned. Otherwise, that CPU
-	 * is stalled because the picked CPU is already punched out from the
-	 * idle mask.
-	 */
 	cpu_id = pick_cpu(p, taskc, prev_cpu, wake_flags, &found_idle);
-	if (found_idle) {
-		put_local_rq_no_fail(p, taskc, 0);
+	if (found_idle)
 		return cpu_id;
-	}
-
-	/*
-	 * If there is no idle CPU, consider to preempt out the current running
-	 * task if there is a higher priority task in the global run queue.
-	 */
-try_yield_out:
-	p_run = bpf_get_current_task_btf();
-	taskc_run = try_get_task_ctx(p_run);
-	if (!taskc_run)
-		goto out; /* it is a real error or p_run is swapper */
-	cpuc_run = get_cpu_ctx_id(scx_bpf_task_cpu(p_run));
-	if (!cpuc_run)
-		goto out;
 
-	try_yield_current_cpu(p_run, cpuc_run, taskc_run);
-out:
 	return prev_cpu;
 }
 
-- 
2.45.2


From 9bc20f9160c906fde31fd79055a63c7f29faf88b Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Wed, 17 Jul 2024 23:46:11 +0900
Subject: [PATCH 06/20] scx_lavd: maintain ineligible runnable tasks separately
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

We now maintain two run queues—an eligible run queue (DSQ) and an
ineligible run queue (rbtree)—sorted by the task's virtual deadline.
When the eligible run queue is empty, or the ineligible run queue has
not been consumed for too long (e.g., 15 msec), a task in the ineligible
run queue is moved to the eligible run queue for execution. With these
two queues, we have a better admission control.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h     |   2 +-
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 207 +++++++++++++++++++++---
 2 files changed, 183 insertions(+), 26 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index 2f9de16..93fc738 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -56,7 +56,7 @@ enum consts {
 	LAVD_TIME_INFINITY_NS		= SCX_SLICE_INF,
 	LAVD_MAX_RETRY			= 4,
 
-	LAVD_TARGETED_LATENCY_NS	= (10 * NSEC_PER_MSEC),
+	LAVD_TARGETED_LATENCY_NS	= (15 * NSEC_PER_MSEC),
 	LAVD_SLICE_MIN_NS		= (30 * NSEC_PER_USEC), /* min time slice */
 	LAVD_SLICE_MAX_NS		= ( 3 * NSEC_PER_MSEC), /* max time slice */
 	LAVD_SLICE_UNDECIDED		= SCX_SLICE_INF,
diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index a62bae2..8275807 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -208,6 +208,20 @@ static volatile int	__sys_stat_idx;
 private(LAVD) struct bpf_cpumask __kptr *active_cpumask; /* CPU mask for active CPUs */
 private(LAVD) struct bpf_cpumask __kptr *ovrflw_cpumask; /* CPU mask for overflow CPUs */
 
+/*
+ * Ineligible runnable queue
+ */
+struct task_node {
+	u64			vdeadline;	/* key */
+	u64			pid;		/* value */
+	struct bpf_rb_node	node;
+};
+
+private(LAVD) struct bpf_spin_lock	ineli_lock;
+private(LAVD) struct bpf_rb_root	ineli_rq __contains(task_node, node);
+static u64				nr_ineli;
+static u64				refill_clk; /* when an eliglble DSQ is refilled */
+
 /*
  * CPU topology
  */
@@ -1390,7 +1404,8 @@ static u64 calc_time_slice(struct task_struct *p, struct task_ctx *taskc)
 	 * The time slice should be short enough to schedule all runnable tasks
 	 * at least once within a targeted latency.
 	 */
-	nr_queued = (u64)scx_bpf_dsq_nr_queued(LAVD_ELIGIBLE_DSQ) + 1;
+	nr_queued = (u64)scx_bpf_dsq_nr_queued(LAVD_ELIGIBLE_DSQ) +
+		    READ_ONCE(nr_ineli) + 1;
 	slice = (LAVD_TARGETED_LATENCY_NS * stat_cur->nr_active) / nr_queued;
 	if (stat_cur->load_factor < 1000 && is_eligible(taskc)) {
 		slice += (LAVD_SLICE_BOOST_MAX_FT * slice *
@@ -1941,11 +1956,23 @@ static bool try_yield_current_cpu(struct task_struct *p_run,
 	return ret;
 }
 
+static bool less(struct bpf_rb_node *a, const struct bpf_rb_node *b)
+{
+	struct task_node *node_a;
+	struct task_node *node_b;
+
+	node_a = container_of(a, struct task_node, node);
+	node_b = container_of(b, struct task_node, node);
+
+	return node_a->vdeadline < node_b->vdeadline;
+}
+
 static void put_global_rq(struct task_struct *p, struct task_ctx *taskc,
 			  struct cpu_ctx *cpuc, u64 enq_flags)
 {
 	struct task_ctx *taskc_run;
 	struct task_struct *p_run;
+	struct task_node *t;
 
 	/*
 	 * Calculate when a tack can be scheduled.
@@ -1956,26 +1983,49 @@ static void put_global_rq(struct task_struct *p, struct task_ctx *taskc,
 	calc_when_to_run(p, taskc, cpuc, enq_flags);
 
 	/*
-	 * Try to find and kick a victim CPU, which runs a less urgent task.
-	 * The kick will be done asynchronously.
+	 * If a task is eligible, dispatch to the eligible DSQ.
 	 */
-	try_find_and_kick_victim_cpu(p->cpus_ptr, taskc);
+	if (is_eligible(taskc)) {
+		/*
+		 * Try to find and kick a victim CPU, which runs a less urgent
+		 * task. The kick will be done asynchronously.
+		 */
+		try_find_and_kick_victim_cpu(p->cpus_ptr, taskc);
+
+		/*
+		 * If the current task has something to yield, try preempt it.
+		 */
+		p_run = bpf_get_current_task_btf();
+		taskc_run = try_get_task_ctx(p_run);
+		if (taskc_run && p_run->scx.slice != 0)
+			try_yield_current_cpu(p_run, cpuc, taskc_run);
+
+		goto dispatch_out;
+	}
 
 	/*
-	 * If the current task has something to yield, try preempt it.
+	 * If the task is ineligible, keep it to the ineligible run queue.
 	 */
-	p_run = bpf_get_current_task_btf();
-	taskc_run = try_get_task_ctx(p_run);
-	if (taskc_run && p_run->scx.slice != 0)
-		try_yield_current_cpu(p_run, cpuc, taskc_run);
+	t = bpf_obj_new(typeof(*t));
+	if (!t)
+		goto dispatch_out;
+
+	t->vdeadline = taskc->vdeadline_log_clk;
+	t->pid = p->pid;
+
+	bpf_spin_lock(&ineli_lock);
+	bpf_rbtree_add(&ineli_rq, &t->node, less);
+	WRITE_ONCE(nr_ineli, nr_ineli + 1);
+	bpf_spin_unlock(&ineli_lock);
+	return;
 
 	/*
-	 * Enqueue the task to the global runqueue based on its virtual
-	 * deadline.
+	 * Enqueue the task to the eligible DSQ based on its virtual deadline.
 	 */
+dispatch_out:
 	scx_bpf_dispatch_vtime(p, LAVD_ELIGIBLE_DSQ, LAVD_SLICE_UNDECIDED,
 			       taskc->vdeadline_log_clk, enq_flags);
-
+	return;
 }
 
 static bool could_run_on_prev(struct task_struct *p, s32 prev_cpu,
@@ -2097,9 +2147,7 @@ s32 BPF_STRUCT_OPS(lavd_select_cpu, struct task_struct *p, s32 prev_cpu,
 		   u64 wake_flags)
 {
 	bool found_idle = false;
-	struct task_struct *p_run;
-	struct cpu_ctx *cpuc_run;
-	struct task_ctx *taskc, *taskc_run;
+	struct task_ctx *taskc;
 	s32 cpu_id;
 	
 	taskc = get_task_ctx(p);
@@ -2150,6 +2198,101 @@ static bool use_full_cpus(void)
 	       ((stat_cur->nr_active + LAVD_TC_NR_OVRFLW) >= nr_cpus_onln);
 }
 
+static bool refill_eligible_dsq(s32 cpu)
+{
+	struct task_struct *p;
+	struct task_ctx *taskc;
+	struct cpu_ctx *cpuc;
+	struct bpf_rb_node *node;
+	struct task_node *t;
+	u64 pid;
+
+	/*
+	 * Fetch the first task on the ineligible rq.
+	 */
+	bpf_spin_lock(&ineli_lock);
+	node = bpf_rbtree_first(&ineli_rq);
+	if (!node) {
+		bpf_spin_unlock(&ineli_lock);
+		return false;
+	}
+	t = container_of(node, struct task_node, node);
+	pid = t->pid;
+
+	node = bpf_rbtree_remove(&ineli_rq, &t->node);
+	WRITE_ONCE(nr_ineli, nr_ineli - 1);
+	bpf_spin_unlock(&ineli_lock);
+
+	if (node) {
+		t = container_of(node, struct task_node, node);
+		bpf_obj_drop(t);
+	}
+
+	/*
+	 * Recalculate when to run for the task.
+	 */
+	p = bpf_task_from_pid(pid);
+	if (!p)
+		return false;
+	taskc = get_task_ctx(p);
+	cpuc = get_cpu_ctx_id(cpu);
+	if (!cpuc || !taskc) {
+		bpf_task_release(p);
+		return false;
+	}
+
+	calc_when_to_run(p, taskc, cpuc, 0);
+
+	/*
+	 * Dispatch the first task to the eligible queue for future
+	 * scheduling.
+	 */
+	scx_bpf_dispatch_vtime(p, LAVD_ELIGIBLE_DSQ,
+			       LAVD_SLICE_UNDECIDED,
+			       taskc->vdeadline_log_clk, 0);
+	bpf_task_release(p);
+
+	/*
+	 * Update the refill clock.
+	 */
+	WRITE_ONCE(refill_clk, bpf_ktime_get_ns());
+
+	return true;
+}
+
+static bool is_time_to_refill(void)
+{
+	return (READ_ONCE(refill_clk) + LAVD_TARGETED_LATENCY_NS) <
+		bpf_ktime_get_ns();
+}
+
+static bool consume_task(s32 cpu)
+{
+	bool ret;
+
+	/*
+	 * If the ineligible rq has not consumed too long,
+	 * move a task from the ineligible rq to the eligible dsq.
+	 */
+	if (is_time_to_refill())
+		refill_eligible_dsq(cpu);
+
+	/*
+	 * Consume a task from the eliglble dsq.
+	 */
+	ret = scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
+	if (!ret) {
+		/*
+		 * If there is not task to run on the eligible dsq, refill it
+		 * then try it again.
+		 */
+		if (refill_eligible_dsq(cpu))
+			ret = scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
+	}
+
+	return ret;
+}
+
 void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 {
 	struct bpf_cpumask *active, *ovrflw;
@@ -2159,7 +2302,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 	 * If all CPUs are using, directly consume without checking CPU masks.
 	 */
 	if (use_full_cpus()) {
-		scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
+		consume_task(cpu);
 		return;
 	}
 
@@ -2180,7 +2323,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 	 */
 	if (bpf_cpumask_test_cpu(cpu, cast_mask(active)) ||
 	    bpf_cpumask_test_cpu(cpu, cast_mask(ovrflw))) {
-		scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
+		consume_task(cpu);
 		goto unlock_out;
 	}
 
@@ -2195,7 +2338,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 		 * kworker, etc).
 		 */
 		if (is_kernel_task(p)) {
-			scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
+			consume_task(cpu);
 			break;
 		}
 
@@ -2227,7 +2370,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 		 * cores. We will optimize this path after introducing per-core
 		 * DSQ.
 		 */
-		scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
+		consume_task(cpu);
 
 		/*
 		 * This is the first time a particular pinned user-space task
@@ -2247,7 +2390,6 @@ release_break:
 unlock_out:
 	bpf_rcu_read_unlock();
 	return;
-
 }
 
 static int calc_cpuperf_target(struct sys_stat *stat_cur,
@@ -2768,6 +2910,23 @@ s32 BPF_STRUCT_OPS(lavd_init_task, struct task_struct *p,
 	return 0;
 }
 
+static s32 init_dsqs(void)
+{
+	int err;
+
+	err = scx_bpf_create_dsq(LAVD_ELIGIBLE_DSQ, -1);
+	if (err) {
+		scx_bpf_error("Failed to create an eligible DSQ");
+		return err;
+	}
+
+	/*
+	 * Nothing to init for ineli_rq.
+	 */
+
+	return 0;
+}
+
 static int calloc_cpumask(struct bpf_cpumask **p_cpumask)
 {
 	struct bpf_cpumask *cpumask;
@@ -2871,13 +3030,11 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_init)
 	int err;
 
 	/*
-	 * Create a central task queue.
+	 * Create central task queues.
 	 */
-	err = scx_bpf_create_dsq(LAVD_ELIGIBLE_DSQ, -1);
-	if (err) {
-		scx_bpf_error("Failed to create a shared DSQ");
+	err = init_dsqs();
+	if (err)
 		return err;
-	}
 
 	/*
 	 * Initialize per-CPU context.
-- 
2.45.2


From 78d96a6fb680d5e6f757553c31a82282a691b3c5 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Thu, 18 Jul 2024 15:53:38 +0900
Subject: [PATCH 07/20] scx_lavd: advance clock by reverse proportional to the
 system load

Advancing the clock slower when overloaded gives more opportunities for
latency-critical tasks to cut in the run queue. Controlling the clock
better reflects the actual load than the prior approach of stretching
the time-space when overloaded.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 46 ++++++++++++++++---------
 1 file changed, 29 insertions(+), 17 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index 8275807..cf3815a 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -1336,20 +1336,6 @@ static u64 calc_virtual_deadline_delta(struct task_struct *p,
 	weight = calc_latency_weight(p, taskc, cpuc, is_wakeup);
 	vdeadline_delta_ns = (((taskc->run_time_ns + 1) * weight) + 1000) / 1000;
 
-	/*
-	 * When a system is overloaded (>1000), stretch time space so make time
-	 * tick logically slower to give room to execute the overloaded tasks.
-	 */
-	if (load_factor > 1000) {
-		/*
-		 * The time space is stretched more if task's latency priority
-		 * is lower (i.e., higher value) and the load is higher.
-		 */
-		vdeadline_delta_ns = (vdeadline_delta_ns * load_factor *
-				      (taskc->lat_prio + 1)) /
-				     (LAVD_LOAD_FACTOR_FT * 1000);
-	}
-
 	taskc->vdeadline_delta_ns = vdeadline_delta_ns;
 
 	return vdeadline_delta_ns;
@@ -1395,6 +1381,11 @@ static u64 clamp_time_slice_ns(u64 slice)
 	return slice;
 }
 
+static u64 rq_nr_queued(void)
+{
+	return scx_bpf_dsq_nr_queued(LAVD_ELIGIBLE_DSQ) + READ_ONCE(nr_ineli);
+}
+
 static u64 calc_time_slice(struct task_struct *p, struct task_ctx *taskc)
 {
 	struct sys_stat *stat_cur = get_sys_stat_cur();
@@ -1404,8 +1395,7 @@ static u64 calc_time_slice(struct task_struct *p, struct task_ctx *taskc)
 	 * The time slice should be short enough to schedule all runnable tasks
 	 * at least once within a targeted latency.
 	 */
-	nr_queued = (u64)scx_bpf_dsq_nr_queued(LAVD_ELIGIBLE_DSQ) +
-		    READ_ONCE(nr_ineli) + 1;
+	nr_queued = rq_nr_queued() + 1;
 	slice = (LAVD_TARGETED_LATENCY_NS * stat_cur->nr_active) / nr_queued;
 	if (stat_cur->load_factor < 1000 && is_eligible(taskc)) {
 		slice += (LAVD_SLICE_BOOST_MAX_FT * slice *
@@ -1472,6 +1462,28 @@ static void update_stat_for_runnable(struct task_struct *p,
 	cpuc->load_run_time_ns += clamp_time_slice_ns(taskc->run_time_ns);
 }
 
+static void advance_cur_logical_clk(struct task_ctx *taskc)
+{
+	u64 nr_queued, delta, new_clk;
+
+	/*
+	 * The clock should not go backward, so do nothing.
+	 */
+	if (taskc->vdeadline_log_clk <= cur_logical_clk) {
+		return;
+	}
+
+	/*
+	 * Advance the clock up to the task's deadline. When overloaded,
+	 * advnace the clock slower so other can jump in the run queue.
+	 */
+	nr_queued = max(rq_nr_queued(), 1);
+	delta = (taskc->vdeadline_log_clk - cur_logical_clk) / nr_queued;
+	new_clk = cur_logical_clk + delta;
+
+	WRITE_ONCE(cur_logical_clk, new_clk);
+}
+
 static void update_stat_for_running(struct task_struct *p,
 				    struct task_ctx *taskc,
 				    struct cpu_ctx *cpuc)
@@ -1484,7 +1496,7 @@ static void update_stat_for_running(struct task_struct *p,
 	/*
 	 * Update the current logical clock.
 	 */
-	WRITE_ONCE(cur_logical_clk, taskc->vdeadline_log_clk);
+	advance_cur_logical_clk(taskc);
 
 	/*
 	 * Update the current service time if necessary.
-- 
2.45.2


From b90599e9676ac3fe42f9f38dcf4366eed3021d69 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Fri, 19 Jul 2024 15:29:13 +0900
Subject: [PATCH 08/20] scx_lavd: do not inherit parent's properties

If inheriting the parent's properties, a new fork task tends to be too
prioritized. That is, many parent processes, such as `make,` are a bit
more latency-critical than average.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 38 +++++--------------------
 1 file changed, 7 insertions(+), 31 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index cf3815a..2667762 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -1315,7 +1315,6 @@ static u64 calc_virtual_deadline_delta(struct task_struct *p,
 				       struct cpu_ctx *cpuc,
 				       u64 enq_flags)
 {
-	u64 load_factor = get_sys_stat_cur()->load_factor;
 	u64 vdeadline_delta_ns, weight;
 	bool is_wakeup;
 
@@ -2833,39 +2832,16 @@ void BPF_STRUCT_OPS(lavd_update_idle, s32 cpu, bool idle)
 
 static void init_task_ctx(struct task_struct *p, struct task_ctx *taskc)
 {
-	struct task_struct *parent;
-	struct task_ctx *taskc_parent;
-	u64 now;
-
-	/*
-	 * Inherit parent's statistics if the parent is also under scx.
-	 */
-	parent = p->parent;
-	taskc_parent = try_get_task_ctx(parent);
-	if (parent && taskc_parent)
-		memcpy(taskc, taskc_parent, sizeof(*taskc));
-	else {
-		/*
-		 * If parent's ctx does not exist, init some fields with
-		 * reasonable defaults.
-		 */
-		taskc->run_time_ns = LAVD_SLICE_MIN_NS;
-		taskc->lat_prio = get_nice_prio(p);
-		taskc->run_freq = 1;
-	}
+	u64 now = bpf_ktime_get_ns();
 
-	/*
-	 * Reset context for a fresh new task.
-	 */
-	now = bpf_ktime_get_ns();
-	taskc->last_runnable_clk = now;
-	taskc->last_running_clk = now;
-	taskc->last_stopping_clk = now;
-	taskc->last_quiescent_clk = now;
+	memset(taskc, 0, sizeof(*taskc));
+	taskc->last_running_clk = now; /* for run_time_ns */
+	taskc->last_stopping_clk = now; /* for run_time_ns */
+	taskc->run_time_ns = LAVD_SLICE_MAX_NS;
+	taskc->lat_prio = get_nice_prio(p);
+	taskc->run_freq = 0;
 	taskc->greedy_ratio = 1000;
 	taskc->victim_cpu = (s32)LAVD_CPU_ID_NONE;
-	taskc->acc_run_time_ns = 0;
-	taskc->slice_ns = 0;
 }
 
 void BPF_STRUCT_OPS(lavd_enable, struct task_struct *p)
-- 
2.45.2


From 99e0d21c3cced1c586a6c20e41348005cf82e111 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Fri, 19 Jul 2024 17:28:40 +0900
Subject: [PATCH 09/20] scx_lavd: drop the runtime factor in calculating
 latency criticality

That is okay since the runtime is considered in calculating a virtual
deadline. A shorter runtime will result in a tighter deadline linearly.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 24 ++++--------------------
 1 file changed, 4 insertions(+), 20 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index 2667762..c0af762 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -1209,7 +1209,7 @@ static int map_lat_cri_to_lat_prio(u32 lat_cri)
 static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
 		     struct cpu_ctx *cpuc, bool is_wakeup)
 {
-	u64 run_time_ft = 0, wait_freq_ft = 0, wake_freq_ft = 0;
+	u64 wait_freq_ft = 0, wake_freq_ft = 0;
 	u64 lat_cri_raw;
 	u16 static_prio;
 	int boost;
@@ -1226,25 +1226,18 @@ static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
 
 	/*
 	 * A task is more latency-critical as its wait or wake frequencies
-	 * (i.e., wait_freq and wake_freq) are higher and/or its runtime per
-	 * schedule (run_time) is shorter.
+	 * (i.e., wait_freq and wake_freq) are higher.
 	 *
 	 * Since those numbers are unbounded and their upper limits are
 	 * unknown, we transform them using sigmoid-like functions. For wait
 	 * and wake frequencies, we use a sigmoid function (sigmoid_u64), which
 	 * is monotonically increasing since higher frequencies mean more
-	 * latency-critical. For per-schedule runtime, we use a horizontally
-	 * flipped version of the sigmoid function (rsigmoid_u64) because a
-	 * shorter runtime means more latency-critical.
+	 * latency-critical.
 	 */
-	run_time_ft = calc_runtime_factor(taskc->run_time_ns);
 	wait_freq_ft = calc_freq_factor(taskc->wait_freq);
 	wake_freq_ft = calc_freq_factor(taskc->wake_freq);
 
 	/*
-	 * A raw latency criticality factor consists of two parts -- a
-	 * frequency part and a runtime part.
-	 *
 	 * Wake frequency and wait frequency represent how much a task is used
 	 * for a producer and a consumer, respectively. If both are high, the
 	 * task is in the middle of a task chain. We multiply frequencies --
@@ -1253,17 +1246,8 @@ static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
 	 * wake_freq to prioritize scheduling of a producer task. That's
 	 * because if the scheduling of a producer task is delayed, all the
 	 * following consumer tasks are also delayed.
-	 *
-	 * For the runtime part, we cubic the runtime to amplify the subtle
-	 * differences.
-	 *
-	 * We aggregate the frequency part and the runtime part using addition.
-	 * In this way, if a task is either high-frequency _or_ short-runtime,
-	 * it is considered latency-critical. Of course, such a task with both
-	 * high frequency _and_ short runtime is _super_ latency-critical.
 	 */
-	lat_cri_raw = (wait_freq_ft * wake_freq_ft * wake_freq_ft) + 
-		      (run_time_ft * run_time_ft * run_time_ft);
+	lat_cri_raw = (wait_freq_ft * wake_freq_ft * wake_freq_ft);
 
 	/*
 	 * The ratio above tends to follow an exponentially skewed
-- 
2.45.2


From 034303f00fa36b8260739ccfa31399e118c70bc8 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Fri, 19 Jul 2024 22:07:32 +0900
Subject: [PATCH 10/20] scx_lavd: consider starvation factor in determining
 latency criticality

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h     |  1 +
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 24 ++++++++++++++++++++----
 2 files changed, 21 insertions(+), 4 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index 93fc738..48ce986 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -67,6 +67,7 @@ enum consts {
 	LAVD_LC_FREQ_MAX		= 1000000,
 	LAVD_LC_RUNTIME_MAX		= LAVD_TARGETED_LATENCY_NS,
 	LAVD_LC_RUNTIME_SHIFT		= 10,
+	LAVD_LC_STARVATION_FT		= 1024,
 
 	LAVD_BOOST_RANGE		= 40, /* 100% of nice range */
 	LAVD_BOOST_WAKEUP_LAT		= 1,
diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index c0af762..04ce3d7 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -1206,10 +1206,23 @@ static int map_lat_cri_to_lat_prio(u32 lat_cri)
 	return lat_prio;
 }
 
+static u64 calc_starvation_factor(struct task_ctx *taskc)
+{
+	struct sys_stat *stat_cur = get_sys_stat_cur();
+	u64 ratio;
+
+	/*
+	 * Prioritize tasks whose service time is smaller than average.
+	 */
+	ratio = (LAVD_LC_STARVATION_FT * stat_cur->avg_svc_time) /
+		taskc->svc_time;
+	return ratio + 1;
+}
+
 static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
 		     struct cpu_ctx *cpuc, bool is_wakeup)
 {
-	u64 wait_freq_ft = 0, wake_freq_ft = 0;
+	u64 starvation_ft, wait_freq_ft, wake_freq_ft;
 	u64 lat_cri_raw;
 	u16 static_prio;
 	int boost;
@@ -1226,9 +1239,9 @@ static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
 
 	/*
 	 * A task is more latency-critical as its wait or wake frequencies
-	 * (i.e., wait_freq and wake_freq) are higher.
+	 * (i.e., wait_freq and wake_freq) and starvation factors are higher.
 	 *
-	 * Since those numbers are unbounded and their upper limits are
+	 * Since those frequencies are unbounded and their upper limits are
 	 * unknown, we transform them using sigmoid-like functions. For wait
 	 * and wake frequencies, we use a sigmoid function (sigmoid_u64), which
 	 * is monotonically increasing since higher frequencies mean more
@@ -1236,6 +1249,7 @@ static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
 	 */
 	wait_freq_ft = calc_freq_factor(taskc->wait_freq);
 	wake_freq_ft = calc_freq_factor(taskc->wake_freq);
+	starvation_ft  = calc_starvation_factor(taskc);
 
 	/*
 	 * Wake frequency and wait frequency represent how much a task is used
@@ -1247,7 +1261,9 @@ static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
 	 * because if the scheduling of a producer task is delayed, all the
 	 * following consumer tasks are also delayed.
 	 */
-	lat_cri_raw = (wait_freq_ft * wake_freq_ft * wake_freq_ft);
+	lat_cri_raw = wait_freq_ft *
+		      wake_freq_ft * wake_freq_ft *
+		      starvation_ft;
 
 	/*
 	 * The ratio above tends to follow an exponentially skewed
-- 
2.45.2


From 6f10d6907c4c30e515451365d278c27744170501 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Fri, 19 Jul 2024 22:39:01 +0900
Subject: [PATCH 11/20] scx_lavd: drop sched_prio_to_slice_weight[] table

Use p->scx.weight instead.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 119 +-----------------------
 1 file changed, 4 insertions(+), 115 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index 04ce3d7..3a5e6dd 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -322,71 +322,6 @@ struct {
 	__uint(max_entries, 16 * 1024 /* 16 KB */);
 } introspec_msg SEC(".maps");
 
-/*
- * A nice priority to CPU usage weight array
- * -----------------------------------------
- *
- * This is the exact same weight array in the kernel (kernel/sched/core.c). We
- * used the same array on purpose to provide the same level of fairness. Each
- * step increases by around 23%. Here is the comments from the kernel source
- * code for reference:
- *
- * "Nice levels are multiplicative, with a gentle 10% change for every nice
- * level changed. I.e. when a CPU-bound task goes from nice 0 to nice 1, it
- * will get ~10% less CPU time than another CPU-bound task that remained on
- * nice 0."
- *
- * "The "10% effect" is relative and cumulative: from _any_ nice level, if you
- * go up 1 level, it's -10% CPU usage, if you go down 1 level it's +10% CPU
- * usage. (to achieve that we use a multiplier of 1.25. If a task goes up by
- * ~10% and another task goes down by ~10% then the relative distance between
- * them is ~25%.)"
- */
-static const u64 sched_prio_to_slice_weight[NICE_WIDTH] = {
-	/* weight	nice priority	sched priority */
-	/* ------	-------------	-------------- */
-	88761,		/* -20		 0 */
-	71755,		/* -19		 1 */
-	56483,		/* -18		 2 */
-	46273,		/* -17		 3 */
-	36291,		/* -16		 4 */
-	29154,		/* -15		 5 */
-	23254,		/* -14		 6 */
-	18705,		/* -13		 7 */
-	14949,		/* -12		 8 */
-	11916,		/* -11		 9 */
-	 9548,		/* -10		10 */
-	 7620,		/*  -9		11 */
-	 6100,		/*  -8		12 */
-	 4904,		/*  -7		13 */
-	 3906,		/*  -6		14 */
-	 3121,		/*  -5		15 */
-	 2501,		/*  -4		16 */
-	 1991,		/*  -3		17 */
-	 1586,		/*  -2		18 */
-	 1277,		/*  -1		19 */
-	 1024,		/*   0		20 */
-	  820,		/*   1		21 */
-	  655,		/*   2		22 */
-	  526,		/*   3		23 */
-	  423,		/*   4		24 */
-	  335,		/*   5		25 */
-	  272,		/*   6		26 */
-	  215,		/*   7		27 */
-	  172,		/*   8		28 */
-	  137,		/*   9		29 */
-	  110,		/*  10		30 */
-	   87,		/*  11		31 */
-	   70,		/*  12		32 */
-	   56,		/*  13		33 */
-	   45,		/*  14		34 */
-	   36,		/*  15		35 */
-	   29,		/*  16		36 */
-	   23,		/*  17		37 */
-	   18,		/*  18		38 */
-	   15,		/*  19		39 */
-};
-
 static u16 get_nice_prio(struct task_struct *p);
 static u64 get_task_load_ideal(struct task_struct *p);
 static void adjust_slice_boost(struct cpu_ctx *cpuc, struct task_ctx *taskc);
@@ -1293,47 +1228,17 @@ out:
 	return boost;
 }
 
-static u64 calc_latency_weight(struct task_struct *p, struct task_ctx *taskc,
-			       struct cpu_ctx *cpuc, bool is_wakeup)
-{
-	int prio = taskc->lat_prio;
-	u64 w;
-
-	boost_lat(p, taskc, cpuc, is_wakeup);
-
-	if (prio >= NICE_WIDTH)
-		prio = NICE_WIDTH - 1;
-	else if (prio < 0)
-		prio = 0;
-
-	w = LAVD_LAT_WEIGHT_FT / sched_prio_to_slice_weight[prio];
-	return w + 1;
-}
-
 static u64 calc_virtual_deadline_delta(struct task_struct *p,
 				       struct task_ctx *taskc,
 				       struct cpu_ctx *cpuc,
 				       u64 enq_flags)
 {
-	u64 vdeadline_delta_ns, weight;
+	u64 vdeadline_delta_ns;
 	bool is_wakeup;
 
-	/* Virtual deadline of @p is defined as follows:
-	 *
-	 *   vdeadline = now + (a full time slice * latency weight)
-	 *
-	 * where
-	 *   - weight is determined by nice priority and boost priorty
-	 *   - (a full time slice * latency weight) determines the time window
-	 *   of competition among concurrent tasks.
-	 *
-	 * Note that not using average runtime (taskc->run_time) is intentional
-	 * because task's average runtime is already reflected in calculating
-	 * boost priority (and weight).
-	 */
 	is_wakeup = is_wakeup_ef(enq_flags);
-	weight = calc_latency_weight(p, taskc, cpuc, is_wakeup);
-	vdeadline_delta_ns = (((taskc->run_time_ns + 1) * weight) + 1000) / 1000;
+	boost_lat(p, taskc, cpuc, is_wakeup);
+	vdeadline_delta_ns = (taskc->run_time_ns * 1000) / taskc->lat_cri;
 
 	taskc->vdeadline_delta_ns = vdeadline_delta_ns;
 
@@ -1342,23 +1247,7 @@ static u64 calc_virtual_deadline_delta(struct task_struct *p,
 
 static u64 get_task_load_ideal(struct task_struct *p)
 {
-	int prio;
-	u64 weight;
-
-	/*
-	 * The task's ideal load is simply the weight based on the task's nice
-	 * priority (without considering boosting). Note that the ideal load
-	 * and actual load are not compatible for comparison. However, the
-	 * ratios of them are directly comparable. 
-	 */
-	prio = get_nice_prio(p);
-	if (prio >= NICE_WIDTH)
-		prio = NICE_WIDTH - 1;
-	else if (prio < 0)
-		prio = 0;
-
-	weight = sched_prio_to_slice_weight[prio];
-	return weight;
+	return p->scx.weight;
 }
 
 static u64 calc_task_load_actual(struct task_ctx *taskc)
-- 
2.45.2


From 67a6deb983547b7d81a155b9d5a78da78694f5b3 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Fri, 19 Jul 2024 23:51:49 +0900
Subject: [PATCH 12/20] scx_lavd: use lat_cri instead of lat_prio universally

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h     |  11 +-
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 162 +++---------------------
 scheds/rust/scx_lavd/src/main.rs        |  24 ++--
 3 files changed, 31 insertions(+), 166 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index 48ce986..cebfde3 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -74,12 +74,10 @@ enum consts {
 	LAVD_SLICE_BOOST_MAX_FT		= 2, /* maximum additional 2x of slice */
 	LAVD_SLICE_BOOST_MAX_STEP	= 8, /* 8 slice exhausitions in a row */
 	LAVD_GREEDY_RATIO_MAX		= USHRT_MAX,
-	LAVD_LAT_PRIO_NEW		= 10,
-	LAVD_LAT_PRIO_IDLE		= USHRT_MAX,
-	LAVD_LAT_WEIGHT_FT		= 88761,
 
 	LAVD_ELIGIBLE_TIME_LAT_FT	= 16,
 	LAVD_ELIGIBLE_TIME_MAX		= (10 * LAVD_TARGETED_LATENCY_NS),
+	LAVD_REFILL_NR			= 2,
 
 	LAVD_CPU_UTIL_MAX		= 1000, /* 100.0% */
 	LAVD_CPU_UTIL_MAX_FOR_CPUPERF	= 850, /* 85.0% */
@@ -87,7 +85,6 @@ enum consts {
 	LAVD_CPU_ID_NONE		= ((u32)-1),
 	LAVD_CPU_ID_MAX			= 512,
 
-	LAVD_PREEMPT_KICK_LAT_PRIO	= 15,
 	LAVD_PREEMPT_KICK_MARGIN	= (2 * NSEC_PER_USEC),
 	LAVD_PREEMPT_TICK_MARGIN	= (1 * NSEC_PER_USEC),
 
@@ -119,9 +116,6 @@ struct sys_stat {
 	volatile u32	min_lat_cri;	/* minimum latency criticality (LC) */
 	volatile u32	thr_lat_cri;	/* latency criticality threshold for kicking */
 
-	volatile s32	inc1k_low;	/* increment from low LC to priority mapping */
-	volatile s32	inc1k_high;	/* increment from high LC to priority mapping */
-
 	volatile u32	avg_perf_cri;	/* average performance criticality */
 
 	volatile u32	nr_violation;	/* number of utilization violation */
@@ -171,7 +165,7 @@ struct cpu_ctx {
 	 * Information of a current running task for preemption
 	 */
 	volatile u64	stopping_tm_est_ns; /* estimated stopping time */
-	volatile u16	lat_prio;	/* latency priority */
+	volatile u16	lat_cri;	/* latency criticality */
 	volatile u8	is_online;	/* is this CPU online? */
 	s32		cpu_id;		/* cpu id */
 
@@ -222,7 +216,6 @@ struct task_ctx {
 	u32	lat_cri;		/* calculated latency criticality */
 	volatile s32 victim_cpu;
 	u16	slice_boost_prio;	/* how many times a task fully consumed the slice */
-	u16	lat_prio;		/* latency priority */
 
 	/*
 	 * Task's performance criticality
diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index 3a5e6dd..ad4eb85 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -308,7 +308,7 @@ struct {
 struct preemption_info {
 	u64		stopping_tm_est_ns;
 	u64		last_kick_clk;
-	u16		lat_prio;
+	u64		lat_cri;
 	struct cpu_ctx	*cpuc;
 };
 
@@ -767,33 +767,6 @@ static void update_sys_stat_next(struct sys_stat_ctx *c)
 				  c->tot_svc_time / c->sched_nr;
 }
 
-static void calc_inc1k(struct sys_stat_ctx *c)
-{
-	/*
-	 * Calculate the increment for mapping from latency criticality to
-	 * priority.
-	 *  - Case 1. inc1k_low:   [min_lc, avg_lc) -> [half_range, 0)
-	 *  - Case 2. inc1k_high:  [avg_lc, max_lc] -> [0, -half_range)
-	 */
-	struct sys_stat *stat_next = c->stat_next;
-
-	if (stat_next->avg_lat_cri == stat_next->min_lat_cri)
-		stat_next->inc1k_low = 0;
-	else {
-		stat_next->inc1k_low = ((LAVD_BOOST_RANGE >> 1) * 1000) /
-					(stat_next->avg_lat_cri -
-					 stat_next->min_lat_cri);
-	}
-
-	if ((stat_next->max_lat_cri + 1) == stat_next->avg_lat_cri)
-		stat_next->inc1k_high = 0;
-	else {	
-		stat_next->inc1k_high = ((LAVD_BOOST_RANGE >> 1) * 1000) /
-					 (stat_next->max_lat_cri + 1 -
-					  stat_next->avg_lat_cri);
-	}
-}
-
 static void do_update_sys_stat(void)
 {
 	struct sys_stat_ctx c;
@@ -805,7 +778,6 @@ static void do_update_sys_stat(void)
 	collect_sys_stat(&c);
 	calc_sys_stat(&c);
 	update_sys_stat_next(&c);
-	calc_inc1k(&c);
 
 	/*
 	 * Make the next version atomically visible.
@@ -997,11 +969,6 @@ static u64 calc_freq_factor(u64 freq)
 	return ft + 1;
 }
 
-static u64 calc_lat_factor(u64 lat_prio)
-{
-	return LAVD_ELIGIBLE_TIME_LAT_FT * (NICE_WIDTH - lat_prio);
-}
-
 static bool is_eligible(struct task_ctx *taskc)
 {
 	return taskc->greedy_ratio <= 1000;
@@ -1042,7 +1009,6 @@ static u64 calc_eligible_delta(struct task_struct *p, struct task_ctx *taskc)
 	 *	when the task become eligible.
 	 */
 	u64 delta_ns;
-	u64 lat_ft;
 
 	/*
 	 * Get how greedy this task has been to enforce fairness if necessary.
@@ -1059,13 +1025,8 @@ static u64 calc_eligible_delta(struct task_struct *p, struct task_ctx *taskc)
 		goto out;
 	}
 
-	/*
-	 * As a task is more latency-critical, it will have a shorter but more
-	 * frequent ineligibility durations.
-	 */
-	lat_ft = calc_lat_factor(taskc->lat_prio);
 	delta_ns = (LAVD_TIME_ONE_SEC / (1000 * taskc->run_freq)) *
-		   (taskc->greedy_ratio / (lat_ft + 1));
+		   (taskc->greedy_ratio);
 
 	if (delta_ns > LAVD_ELIGIBLE_TIME_MAX)
 		delta_ns = LAVD_ELIGIBLE_TIME_MAX;
@@ -1092,55 +1053,6 @@ static int sum_prios_for_lat(struct task_struct *p, int nice_prio,
 	return prio;
 }
 
-static int map_lat_cri_to_lat_prio(u32 lat_cri)
-{
-	/*
-	 * Latency criticality is an absolute metric representing how
-	 * latency-critical a task is. However, latency priority is a relative
-	 * metric compared to the other co-running tasks. Especially when the
-	 * task's latency criticalities are in a small range, the relative
-	 * metric is advantageous in mitigating integer truncation errors. In
-	 * the relative metric, we map
-	 *
-	 *  - Case 1. inc1k_low:   [min_lc, avg_lc) -> [boost_range/2,  0)
-	 *  - Case 2. inc1k_high:  [avg_lc, max_lc] -> [0, -boost_range/2)
-	 *
-	 * Hence, latency priority 20 now means that a task has an average
-	 * latency criticality among the co-running tasks.
-	 */
-
-	struct sys_stat *stat_cur = get_sys_stat_cur();
-	s32 base_lat_cri, inc1k;
-	int base_prio, lat_prio;
-
-	/*
-	 * Set up params for the Case 1 and 2.
-	 */
-	if (lat_cri < stat_cur->avg_lat_cri) {
-		inc1k = stat_cur->inc1k_low;
-		base_lat_cri = stat_cur->min_lat_cri;
-		base_prio = LAVD_BOOST_RANGE >> 1;
-	}
-	else {
-		inc1k = stat_cur->inc1k_high;
-		base_lat_cri = stat_cur->avg_lat_cri;
-		base_prio = 0;
-	}
-
-	/*
-	 * Task's lat_cri could be more up-to-date than stat_cur's one. In this
-	 * case, just take the stat_cur's one.
-	 */
-	if (lat_cri >= base_lat_cri) {
-		lat_prio = base_prio -
-			   (((lat_cri - base_lat_cri) * inc1k + 500) / 1000);
-	}
-	else
-		lat_prio = base_prio;
-
-	return lat_prio;
-}
-
 static u64 calc_starvation_factor(struct task_ctx *taskc)
 {
 	struct sys_stat *stat_cur = get_sys_stat_cur();
@@ -1154,23 +1066,11 @@ static u64 calc_starvation_factor(struct task_ctx *taskc)
 	return ratio + 1;
 }
 
-static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
-		     struct cpu_ctx *cpuc, bool is_wakeup)
+static void boost_lat(struct task_struct *p, struct task_ctx *taskc,
+		      struct cpu_ctx *cpuc, bool is_wakeup)
 {
 	u64 starvation_ft, wait_freq_ft, wake_freq_ft;
 	u64 lat_cri_raw;
-	u16 static_prio;
-	int boost;
-
-	/*
-	 * If a task has yet to be scheduled (i.e., a freshly forked task or a
-	 * task just under sched_ext), don't boost its priority before knowing
-	 * its property.
-	 */
-	if (!have_scheduled(taskc)) {
-		boost = LAVD_LAT_PRIO_NEW;
-		goto out;
-	}
 
 	/*
 	 * A task is more latency-critical as its wait or wake frequencies
@@ -1189,16 +1089,9 @@ static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
 	/*
 	 * Wake frequency and wait frequency represent how much a task is used
 	 * for a producer and a consumer, respectively. If both are high, the
-	 * task is in the middle of a task chain. We multiply frequencies --
-	 * wait_freq * wake_freq * wake_freq -- to amplify the subtle
-	 * differences in frequencies than simple addition. Also, we square
-	 * wake_freq to prioritize scheduling of a producer task. That's
-	 * because if the scheduling of a producer task is delayed, all the
-	 * following consumer tasks are also delayed.
+	 * task is in the middle of a task chain.
 	 */
-	lat_cri_raw = wait_freq_ft *
-		      wake_freq_ft * wake_freq_ft *
-		      starvation_ft;
+	lat_cri_raw = wait_freq_ft * wake_freq_ft * starvation_ft;
 
 	/*
 	 * The ratio above tends to follow an exponentially skewed
@@ -1211,21 +1104,7 @@ static int boost_lat(struct task_struct *p, struct task_ctx *taskc,
 	 * conversion, we mitigate the exponentially skewed distribution to
 	 * non-linear distribution.
 	 */
-	taskc->lat_cri = log2_u64(lat_cri_raw + 1);
-
-	/*
-	 * Convert @p's latency criticality to its boost priority linearly.
-	 * When a task is wakening up, boost its latency boost priority by 1.
-	 */
-	boost = map_lat_cri_to_lat_prio(taskc->lat_cri);
-	if (is_wakeup)
-		boost -= LAVD_BOOST_WAKEUP_LAT;
-
-out:
-	static_prio = get_nice_prio(p);
-	taskc->lat_prio = sum_prios_for_lat(p, static_prio, boost);
-
-	return boost;
+	taskc->lat_cri = log2_u64(lat_cri_raw + 1) + is_wakeup;
 }
 
 static u64 calc_virtual_deadline_delta(struct task_struct *p,
@@ -1556,10 +1435,10 @@ static int comp_preemption_info(struct preemption_info *prm_a,
 	/*
 	 * Check if one's latency priority _or_ deadline is smaller or not.
 	 */
-	if ((prm_a->lat_prio < prm_b->lat_prio) ||
+	if ((prm_a->lat_cri < prm_b->lat_cri) ||
 	    (prm_a->stopping_tm_est_ns < prm_b->stopping_tm_est_ns))
 		return -1;
-	if ((prm_a->lat_prio > prm_b->lat_prio) ||
+	if ((prm_a->lat_cri > prm_b->lat_cri) ||
 	    (prm_a->stopping_tm_est_ns > prm_b->stopping_tm_est_ns))
 		return 1;
 	return 0;
@@ -1593,7 +1472,7 @@ static  bool can_cpu1_kick_cpu2(struct preemption_info *prm_cpu1,
 	 * Set a CPU information
 	 */
 	prm_cpu2->stopping_tm_est_ns = cpuc2->stopping_tm_est_ns;
-	prm_cpu2->lat_prio = cpuc2->lat_prio;
+	prm_cpu2->lat_cri = cpuc2->lat_cri;
 	prm_cpu2->cpuc = cpuc2;
 	prm_cpu2->last_kick_clk = cpuc2->last_kick_clk;
 
@@ -1613,12 +1492,8 @@ static bool is_worth_kick_other_task(struct task_ctx *taskc)
 	 * enough.
 	 */
 	struct sys_stat *stat_cur = get_sys_stat_cur();
-	bool ret;
-
-	ret = (taskc->lat_prio <= LAVD_PREEMPT_KICK_LAT_PRIO) &&
-	      (taskc->lat_cri >= stat_cur->thr_lat_cri);
 
-	return ret;
+	return (taskc->lat_cri >= stat_cur->thr_lat_cri);
 }
 
 static bool can_cpu_be_kicked(u64 now, struct cpu_ctx *cpuc)
@@ -1652,7 +1527,7 @@ static struct cpu_ctx *find_victim_cpu(const struct cpumask *cpumask,
 	 */
 	prm_task.stopping_tm_est_ns = get_est_stopping_time(taskc) +
 				      LAVD_PREEMPT_KICK_MARGIN;
-	prm_task.lat_prio = taskc->lat_prio;
+	prm_task.lat_cri = taskc->lat_cri;
 	prm_task.cpuc = cpuc = get_cpu_ctx();
 	if (!cpuc) {
 		scx_bpf_error("Failed to lookup the current cpu_ctx");
@@ -1819,7 +1694,7 @@ static bool try_yield_current_cpu(struct task_struct *p_run,
 	prm_run.stopping_tm_est_ns = taskc_run->last_running_clk +
 				     taskc_run->run_time_ns -
 				     LAVD_PREEMPT_TICK_MARGIN;
-	prm_run.lat_prio = taskc_run->lat_prio;
+	prm_run.lat_cri = taskc_run->lat_cri;
 
 	bpf_rcu_read_lock();
 	bpf_for_each(scx_dsq, p_wait, LAVD_ELIGIBLE_DSQ, 0) {
@@ -1832,7 +1707,7 @@ static bool try_yield_current_cpu(struct task_struct *p_run,
 			break;
 
 		prm_wait.stopping_tm_est_ns = get_est_stopping_time(taskc_wait);
-		prm_wait.lat_prio = taskc_wait->lat_prio;
+		prm_wait.lat_cri = taskc_wait->lat_cri;
 
 		if (can_task1_kick_task2(&prm_wait, &prm_run)) {
 			/*
@@ -2506,7 +2381,7 @@ void BPF_STRUCT_OPS(lavd_running, struct task_struct *p)
 	/*
 	 * Update running task's information for preemption
 	 */
-	cpuc->lat_prio = taskc->lat_prio;
+	cpuc->lat_cri = taskc->lat_cri;
 	cpuc->stopping_tm_est_ns = get_est_stopping_time(taskc);
 
 	/*
@@ -2613,7 +2488,7 @@ static void cpu_ctx_init_online(struct cpu_ctx *cpuc, u32 cpu_id, u64 now)
 {
 	cpuc->idle_start_clk = 0;
 	cpuc->cpu_id = cpu_id;
-	cpuc->lat_prio = LAVD_LAT_PRIO_IDLE;
+	cpuc->lat_cri = 0;
 	cpuc->stopping_tm_est_ns = LAVD_TIME_INFINITY_NS;
 	WRITE_ONCE(cpuc->online_clk, now);
 	barrier();
@@ -2629,7 +2504,7 @@ static void cpu_ctx_init_offline(struct cpu_ctx *cpuc, u32 cpu_id, u64 now)
 	cpuc->is_online = false;
 	barrier();
 
-	cpuc->lat_prio = LAVD_LAT_PRIO_IDLE;
+	cpuc->lat_cri = 0;
 	cpuc->stopping_tm_est_ns = LAVD_TIME_INFINITY_NS;
 }
 
@@ -2690,7 +2565,7 @@ void BPF_STRUCT_OPS(lavd_update_idle, s32 cpu, bool idle)
 	 */
 	if (idle) {
 		cpuc->idle_start_clk = bpf_ktime_get_ns();
-		cpuc->lat_prio = LAVD_LAT_PRIO_IDLE;
+		cpuc->lat_cri = 0;
 		cpuc->stopping_tm_est_ns = LAVD_TIME_INFINITY_NS;
 	}
 	/*
@@ -2727,7 +2602,6 @@ static void init_task_ctx(struct task_struct *p, struct task_ctx *taskc)
 	taskc->last_running_clk = now; /* for run_time_ns */
 	taskc->last_stopping_clk = now; /* for run_time_ns */
 	taskc->run_time_ns = LAVD_SLICE_MAX_NS;
-	taskc->lat_prio = get_nice_prio(p);
 	taskc->run_freq = 0;
 	taskc->greedy_ratio = 1000;
 	taskc->victim_cpu = (s32)LAVD_CPU_ID_NONE;
diff --git a/scheds/rust/scx_lavd/src/main.rs b/scheds/rust/scx_lavd/src/main.rs
index 4e3ead5..4b0ba3d 100644
--- a/scheds/rust/scx_lavd/src/main.rs
+++ b/scheds/rust/scx_lavd/src/main.rs
@@ -190,13 +190,13 @@ impl<'a> Scheduler<'a> {
         if mseq % 32 == 1 {
             info!(
                 "| {:6} | {:7} | {:17} \
-                   | {:4} | {:4} | {:9} \
+                   | {:4} | {:4} | {:12} \
                    | {:14} | {:8} | {:7} \
-                   | {:8} | {:4} | {:7} \
-                   | {:8} | {:7} | {:9} \
-                   | {:9} | {:9} | {:9} \
+                   | {:8} | {:7} | {:8} \
+                   | {:7} | {:9} | {:9} \
+                   | {:9} | {:9} | {:8} \
                    | {:8} | {:8} | {:8} \
-                   | {:8} | {:6} | {:6} |",
+                   | {:6} | {:6} |",
                 "mseq",
                 "pid",
                 "comm",
@@ -206,8 +206,7 @@ impl<'a> Scheduler<'a> {
                 "eli_ns",
                 "slc_ns",
                 "grdy_rt",
-                "lat_prio",
-                "lc",
+                "lat_cri",
                 "avg_lc",
                 "st_prio",
                 "slc_bst",
@@ -230,13 +229,13 @@ impl<'a> Scheduler<'a> {
 
         info!(
             "| {:6} | {:7} | {:17} \
-               | {:4} | {:4} | {:9} \
+               | {:4} | {:4} | {:12} \
                | {:14} | {:8} | {:7} \
-               | {:8} | {:4} | {:7} \
-               | {:8} | {:7} | {:9} \
-               | {:9} | {:9} | {:9} \
+               | {:8} | {:7} | {:8} \
+               | {:7} | {:9} | {:9} \
+               | {:9} | {:9} | {:8} \
                | {:8} | {:8} | {:8} \
-               | {:8} | {:6} | {:6} |",
+               | {:6} | {:6} |",
             mseq,
             tx.pid,
             tx_comm,
@@ -246,7 +245,6 @@ impl<'a> Scheduler<'a> {
             tc.eligible_delta_ns,
             tc.slice_ns,
             tc.greedy_ratio,
-            tc.lat_prio,
             tc.lat_cri,
             tx.avg_lat_cri,
             tx.static_prio,
-- 
2.45.2


From c955caefd83d02262ffbf162eafe8f726960ca31 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Sat, 20 Jul 2024 00:10:29 +0900
Subject: [PATCH 13/20] scx_lavd: drop sys_load_factor

In theory, sys_load_factor should not be necessary since we do not
stretch the time space anymore.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h     |  2 --
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 26 +------------------------
 scheds/rust/scx_lavd/src/main.rs        |  6 ++----
 3 files changed, 3 insertions(+), 31 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index cebfde3..7324339 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -105,7 +105,6 @@ enum consts {
 struct sys_stat {
 	volatile u64	last_update_clk;
 	volatile u64	util;		/* average of the CPU utilization */
-	volatile u64	load_factor;	/* system load in % (1000 = 100%) for running all runnables within a LAVD_TARGETED_LATENCY_NS */
 
 	volatile u64	load_ideal;	/* average ideal load of runnable tasks */
 	volatile u64	load_actual;	/* average actual load of runnable tasks */
@@ -229,7 +228,6 @@ struct task_ctx_x {
 	u16	static_prio;	/* nice priority */
 	u32	cpu_id;		/* where a task ran */
 	u64	cpu_util;	/* cpu utilization in [0..100] */
-	u64	sys_load_factor; /* system load factor in [0..100..] */
 	u32	avg_perf_cri;	/* average performance criticality */
 	u32	avg_lat_cri;	/* average latency criticality */
 	u32	nr_active;	/* number of active cores */
diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index ad4eb85..3b6ea8f 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -440,7 +440,6 @@ int submit_task_ctx(struct task_struct *p, struct task_ctx *taskc, u32 cpu_id)
 	memcpy(m->taskc_x.comm, p->comm, TASK_COMM_LEN);
 	m->taskc_x.static_prio = get_nice_prio(p);
 	m->taskc_x.cpu_util = cpuc->util / 10;
-	m->taskc_x.sys_load_factor = stat_cur->load_factor / 10;
 	m->taskc_x.cpu_id = cpu_id;
 	m->taskc_x.avg_lat_cri = stat_cur->avg_lat_cri;
 	m->taskc_x.avg_perf_cri = stat_cur->avg_perf_cri;
@@ -603,7 +602,6 @@ struct sys_stat_ctx {
 	u64		sum_perf_cri;
 	u32		avg_perf_cri;
 	u64		new_util;
-	u64		new_load_factor;
 	u32		nr_violation;
 };
 
@@ -710,12 +708,6 @@ static void calc_sys_stat(struct sys_stat_ctx *c)
 	c->new_util = (c->compute_total * LAVD_CPU_UTIL_MAX) /
 		      c->duration_total;
 
-	c->new_load_factor = (1000 * LAVD_LOAD_FACTOR_ADJ *
-				c->load_run_time_ns) /
-				(LAVD_TARGETED_LATENCY_NS * nr_cpus_onln);
-	if (c->new_load_factor > LAVD_LOAD_FACTOR_MAX)
-		c->new_load_factor = LAVD_LOAD_FACTOR_MAX;
-
 	if (c->sched_nr == 0) {
 		/*
 		 * When a system is completely idle, it is indeed possible
@@ -746,8 +738,6 @@ static void update_sys_stat_next(struct sys_stat_ctx *c)
 		calc_avg(stat_cur->load_ideal, c->load_ideal);
 	stat_next->util =
 		calc_avg(stat_cur->util, c->new_util);
-	stat_next->load_factor =
-		calc_avg(stat_cur->load_factor, c->new_load_factor);
 
 	stat_next->min_lat_cri =
 		calc_avg32(stat_cur->min_lat_cri, c->min_lat_cri);
@@ -1164,7 +1154,7 @@ static u64 calc_time_slice(struct task_struct *p, struct task_ctx *taskc)
 	 */
 	nr_queued = rq_nr_queued() + 1;
 	slice = (LAVD_TARGETED_LATENCY_NS * stat_cur->nr_active) / nr_queued;
-	if (stat_cur->load_factor < 1000 && is_eligible(taskc)) {
+	if (is_eligible(taskc)) {
 		slice += (LAVD_SLICE_BOOST_MAX_FT * slice *
 			  taskc->slice_boost_prio) / LAVD_SLICE_BOOST_MAX_STEP;
 	}
@@ -1389,19 +1379,6 @@ static void update_stat_for_quiescent(struct task_struct *p,
 	cpuc->load_run_time_ns -= clamp_time_slice_ns(taskc->run_time_ns);
 }
 
-static u64 calc_exclusive_run_window(void)
-{
-	u64 load_factor;
-
-	load_factor = get_sys_stat_cur()->load_factor;
-	if (load_factor > 1000) {
-		return (LAVD_SLICE_MIN_NS *
-			load_factor * load_factor * load_factor) /
-		       (1000 * 1000 * 1000);
-	}
-	return 0;
-}
-
 static void calc_when_to_run(struct task_struct *p, struct task_ctx *taskc,
 			     struct cpu_ctx *cpuc, u64 enq_flags)
 {
@@ -1419,7 +1396,6 @@ static void calc_when_to_run(struct task_struct *p, struct task_ctx *taskc,
 	 * ineligible duration.
 	 */
 	taskc->vdeadline_log_clk = READ_ONCE(cur_logical_clk) +
-				   calc_exclusive_run_window() +
 				   taskc->eligible_delta_ns +
 				   taskc->vdeadline_delta_ns;
 }
diff --git a/scheds/rust/scx_lavd/src/main.rs b/scheds/rust/scx_lavd/src/main.rs
index 4b0ba3d..cffffa9 100644
--- a/scheds/rust/scx_lavd/src/main.rs
+++ b/scheds/rust/scx_lavd/src/main.rs
@@ -196,7 +196,7 @@ impl<'a> Scheduler<'a> {
                    | {:7} | {:9} | {:9} \
                    | {:9} | {:9} | {:8} \
                    | {:8} | {:8} | {:8} \
-                   | {:6} | {:6} |",
+                   | {:6} |",
                 "mseq",
                 "pid",
                 "comm",
@@ -218,7 +218,6 @@ impl<'a> Scheduler<'a> {
                 "avg_pc",
                 "cpufreq",
                 "cpu_util",
-                "sys_ld",
                 "nr_act",
             );
         }
@@ -235,7 +234,7 @@ impl<'a> Scheduler<'a> {
                | {:7} | {:9} | {:9} \
                | {:9} | {:9} | {:8} \
                | {:8} | {:8} | {:8} \
-               | {:6} | {:6} |",
+               | {:6} |",
             mseq,
             tx.pid,
             tx_comm,
@@ -257,7 +256,6 @@ impl<'a> Scheduler<'a> {
             tx.avg_perf_cri,
             tx.cpuperf_cur,
             tx.cpu_util,
-            tx.sys_load_factor,
             tx.nr_active,
         );
 
-- 
2.45.2


From 02ad43d116e77925627b1df8be95be6f298e5bdc Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Sat, 20 Jul 2024 00:25:11 +0900
Subject: [PATCH 14/20] scx_lavd: directly use p->scx.weight instead load_ideal

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h     |  2 --
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 29 ++-----------------------
 2 files changed, 2 insertions(+), 29 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index 7324339..2f08d04 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -106,7 +106,6 @@ struct sys_stat {
 	volatile u64	last_update_clk;
 	volatile u64	util;		/* average of the CPU utilization */
 
-	volatile u64	load_ideal;	/* average ideal load of runnable tasks */
 	volatile u64	load_actual;	/* average actual load of runnable tasks */
 	volatile u64	avg_svc_time;	/* average service time per task */
 
@@ -135,7 +134,6 @@ struct cpu_ctx {
 	/*
 	 * Information used to keep track of load
 	 */
-	volatile u64	load_ideal;	/* ideal loaf of runnable tasks */
 	volatile u64	load_actual;	/* actual load of runnable tasks */
 	volatile u64	load_run_time_ns; /* total runtime of runnable tasks */
 	volatile u64	tot_svc_time;	/* total service time on a CPU */
diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index 3b6ea8f..5900043 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -323,7 +323,6 @@ struct {
 } introspec_msg SEC(".maps");
 
 static u16 get_nice_prio(struct task_struct *p);
-static u64 get_task_load_ideal(struct task_struct *p);
 static void adjust_slice_boost(struct cpu_ctx *cpuc, struct task_ctx *taskc);
 
 static u64 sigmoid_u64(u64 v, u64 max)
@@ -591,7 +590,6 @@ struct sys_stat_ctx {
 	u64		idle_total;
 	u64		compute_total;
 	u64		load_actual;
-	u64		load_ideal;
 	u64		tot_svc_time;
 	u64		load_run_time_ns;
 	s32		max_lat_cri;
@@ -630,7 +628,6 @@ static void collect_sys_stat(struct sys_stat_ctx *c)
 		/*
 		 * Accumulate cpus' loads.
 		 */
-		c->load_ideal += cpuc->load_ideal;
 		c->load_actual += cpuc->load_actual;
 		c->load_run_time_ns += cpuc->load_run_time_ns;
 		c->tot_svc_time += cpuc->tot_svc_time;
@@ -734,8 +731,6 @@ static void update_sys_stat_next(struct sys_stat_ctx *c)
 
 	stat_next->load_actual =
 		calc_avg(stat_cur->load_actual, c->load_actual);
-	stat_next->load_ideal =
-		calc_avg(stat_cur->load_ideal, c->load_ideal);
 	stat_next->util =
 		calc_avg(stat_cur->util, c->new_util);
 
@@ -1114,11 +1109,6 @@ static u64 calc_virtual_deadline_delta(struct task_struct *p,
 	return vdeadline_delta_ns;
 }
 
-static u64 get_task_load_ideal(struct task_struct *p)
-{
-	return p->scx.weight;
-}
-
 static u64 calc_task_load_actual(struct task_ctx *taskc)
 {
 	/*
@@ -1214,7 +1204,6 @@ static void update_stat_for_runnable(struct task_struct *p,
 	 */
 	taskc->load_actual = calc_task_load_actual(taskc);
 	taskc->acc_run_time_ns = 0;
-	cpuc->load_ideal  += get_task_load_ideal(p);
 	cpuc->load_actual += taskc->load_actual;
 	cpuc->load_run_time_ns += clamp_time_slice_ns(taskc->run_time_ns);
 }
@@ -1247,7 +1236,7 @@ static void update_stat_for_running(struct task_struct *p,
 {
 	u64 wait_period, interval;
 	u64 now = bpf_ktime_get_ns();
-	u64 load_actual_ft, load_ideal_ft, wait_freq_ft, wake_freq_ft;
+	u64 load_actual_ft, wait_freq_ft, wake_freq_ft;
 	u64 perf_cri_raw;
 
 	/*
@@ -1303,10 +1292,9 @@ static void update_stat_for_running(struct task_struct *p,
 	 * skewed distribution.
 	 */
 	load_actual_ft = calc_runtime_factor(taskc->load_actual);
-	load_ideal_ft = get_task_load_ideal(p);
 	wait_freq_ft = calc_freq_factor(taskc->wait_freq);
 	wake_freq_ft = calc_freq_factor(taskc->wake_freq);
-	perf_cri_raw = load_actual_ft * load_ideal_ft *
+	perf_cri_raw = load_actual_ft * p->scx.weight *
 		       wait_freq_ft * wake_freq_ft;
 	taskc->perf_cri = log2_u64(perf_cri_raw + 1);
 	cpuc->sum_perf_cri += taskc->perf_cri;
@@ -1374,7 +1362,6 @@ static void update_stat_for_quiescent(struct task_struct *p,
 	 * When quiescent, reduce the per-CPU task load. Per-CPU task load will
 	 * be aggregated periodically at update_sys_cpu_load().
 	 */
-	cpuc->load_ideal  -= get_task_load_ideal(p);
 	cpuc->load_actual -= taskc->load_actual;
 	cpuc->load_run_time_ns -= clamp_time_slice_ns(taskc->run_time_ns);
 }
@@ -2622,18 +2609,6 @@ s32 BPF_STRUCT_OPS(lavd_init_task, struct task_struct *p,
 	 */
 	init_task_ctx(p, taskc);
 
-	/*
-	 * When a task is forked, we immediately reflect changes to the current
-	 * ideal load not to over-allocate time slices without counting forked
-	 * tasks.
-	 */
-	if (args->fork) {
-		struct sys_stat *stat_cur = get_sys_stat_cur();
-		u64 load_ideal = get_task_load_ideal(p);
-
-		__sync_fetch_and_add(&stat_cur->load_ideal, load_ideal);
-	}
-
 	return 0;
 }
 
-- 
2.45.2


From 3924ebaa4d44d4528601848500dea937c044e6bb Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Sat, 20 Jul 2024 01:41:29 +0900
Subject: [PATCH 15/20] scx_lavd: properly synchronize taskc->vdeadline_log_clk

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 19 +++++++++++--------
 1 file changed, 11 insertions(+), 8 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index 5900043..bfccff4 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -1210,22 +1210,24 @@ static void update_stat_for_runnable(struct task_struct *p,
 
 static void advance_cur_logical_clk(struct task_ctx *taskc)
 {
+	u64 vlc, clc;
 	u64 nr_queued, delta, new_clk;
 
 	/*
 	 * The clock should not go backward, so do nothing.
 	 */
-	if (taskc->vdeadline_log_clk <= cur_logical_clk) {
+	vlc = READ_ONCE(taskc->vdeadline_log_clk);
+	clc = READ_ONCE(cur_logical_clk);
+	if (vlc <= clc)
 		return;
-	}
 
 	/*
 	 * Advance the clock up to the task's deadline. When overloaded,
 	 * advnace the clock slower so other can jump in the run queue.
 	 */
 	nr_queued = max(rq_nr_queued(), 1);
-	delta = (taskc->vdeadline_log_clk - cur_logical_clk) / nr_queued;
-	new_clk = cur_logical_clk + delta;
+	delta = (vlc - clc) / nr_queued;
+	new_clk = clc + delta;
 
 	WRITE_ONCE(cur_logical_clk, new_clk);
 }
@@ -1347,7 +1349,6 @@ static void update_stat_for_stopping(struct task_struct *p,
 	cpuc->load_run_time_ns = cpuc->load_run_time_ns -
 				 clamp_time_slice_ns(old_run_time_ns) +
 				 clamp_time_slice_ns(taskc->run_time_ns);
-
 	/*
 	 * Increase total service time of this CPU.
 	 */
@@ -1369,6 +1370,8 @@ static void update_stat_for_quiescent(struct task_struct *p,
 static void calc_when_to_run(struct task_struct *p, struct task_ctx *taskc,
 			     struct cpu_ctx *cpuc, u64 enq_flags)
 {
+	u64 vlc;
+
 	/*
 	 * Before enqueueing a task to a run queue, we should decide when a
 	 * task should be scheduled. It is determined by two factors: how
@@ -1382,9 +1385,9 @@ static void calc_when_to_run(struct task_struct *p, struct task_ctx *taskc,
 	 * Update the logical clock of the virtual deadline including
 	 * ineligible duration.
 	 */
-	taskc->vdeadline_log_clk = READ_ONCE(cur_logical_clk) +
-				   taskc->eligible_delta_ns +
-				   taskc->vdeadline_delta_ns;
+	vlc = READ_ONCE(cur_logical_clk) + taskc->eligible_delta_ns +
+	      taskc->vdeadline_delta_ns;
+	WRITE_ONCE(taskc->vdeadline_log_clk, vlc);
 }
 
 static u64 get_est_stopping_time(struct task_ctx *taskc)
-- 
2.45.2


From 43f0fcb87c9896f192ca6695fb39e55fd2f40ee1 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Sat, 20 Jul 2024 11:51:12 +0900
Subject: [PATCH 16/20] scx_lavd: removed unused LAVD_LOAD_FACTOR_*

These are no longer necessary after remnoving load factor calculation.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h | 3 ---
 1 file changed, 3 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index 2f08d04..8d672d2 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -60,9 +60,6 @@ enum consts {
 	LAVD_SLICE_MIN_NS		= (30 * NSEC_PER_USEC), /* min time slice */
 	LAVD_SLICE_MAX_NS		= ( 3 * NSEC_PER_MSEC), /* max time slice */
 	LAVD_SLICE_UNDECIDED		= SCX_SLICE_INF,
-	LAVD_LOAD_FACTOR_ADJ		= 6, /* adjustment for better estimation */
-	LAVD_LOAD_FACTOR_MAX		= (20 * 1000),
-	LAVD_LOAD_FACTOR_FT		= 4, /* factor to stretch the time line */
 
 	LAVD_LC_FREQ_MAX		= 1000000,
 	LAVD_LC_RUNTIME_MAX		= LAVD_TARGETED_LATENCY_NS,
-- 
2.45.2


From e94070d5ca29eb1d5243733b217131c057461023 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Sat, 20 Jul 2024 11:53:20 +0900
Subject: [PATCH 17/20] scx_lavd: remove LAVD_BOOST_*

These are no longer necessary after directly using latency criticality.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h | 2 --
 1 file changed, 2 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index 8d672d2..47700ae 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -66,8 +66,6 @@ enum consts {
 	LAVD_LC_RUNTIME_SHIFT		= 10,
 	LAVD_LC_STARVATION_FT		= 1024,
 
-	LAVD_BOOST_RANGE		= 40, /* 100% of nice range */
-	LAVD_BOOST_WAKEUP_LAT		= 1,
 	LAVD_SLICE_BOOST_MAX_FT		= 2, /* maximum additional 2x of slice */
 	LAVD_SLICE_BOOST_MAX_STEP	= 8, /* 8 slice exhausitions in a row */
 	LAVD_GREEDY_RATIO_MAX		= USHRT_MAX,
-- 
2.45.2


From c653622ed903b65b215dca45d2fd497621a1818e Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Sat, 20 Jul 2024 12:00:50 +0900
Subject: [PATCH 18/20] scx_lavd: add LAVD_VDL_LOOSENESS_FT in calculating
 virtual deadline

LAVD_VDL_LOOSENESS_FT represents how loose the deadline is. The smaller
value means the deadline is tighter. While it is unlikely to be tuned,
let's keep it as a tunable for now.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h     |  1 +
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 16 ++++++----------
 2 files changed, 7 insertions(+), 10 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index 47700ae..e50209f 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -60,6 +60,7 @@ enum consts {
 	LAVD_SLICE_MIN_NS		= (30 * NSEC_PER_USEC), /* min time slice */
 	LAVD_SLICE_MAX_NS		= ( 3 * NSEC_PER_MSEC), /* max time slice */
 	LAVD_SLICE_UNDECIDED		= SCX_SLICE_INF,
+	LAVD_VDL_LOOSENESS_FT		= 100,
 
 	LAVD_LC_FREQ_MAX		= 1000000,
 	LAVD_LC_RUNTIME_MAX		= LAVD_TARGETED_LATENCY_NS,
diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index bfccff4..c13e71f 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -1092,21 +1092,17 @@ static void boost_lat(struct task_struct *p, struct task_ctx *taskc,
 	taskc->lat_cri = log2_u64(lat_cri_raw + 1) + is_wakeup;
 }
 
-static u64 calc_virtual_deadline_delta(struct task_struct *p,
-				       struct task_ctx *taskc,
-				       struct cpu_ctx *cpuc,
-				       u64 enq_flags)
+static void calc_virtual_deadline_delta(struct task_struct *p,
+					struct task_ctx *taskc,
+					struct cpu_ctx *cpuc,
+					u64 enq_flags)
 {
-	u64 vdeadline_delta_ns;
 	bool is_wakeup;
 
 	is_wakeup = is_wakeup_ef(enq_flags);
 	boost_lat(p, taskc, cpuc, is_wakeup);
-	vdeadline_delta_ns = (taskc->run_time_ns * 1000) / taskc->lat_cri;
-
-	taskc->vdeadline_delta_ns = vdeadline_delta_ns;
-
-	return vdeadline_delta_ns;
+	taskc->vdeadline_delta_ns = (taskc->run_time_ns *
+				     LAVD_VDL_LOOSENESS_FT) / taskc->lat_cri;
 }
 
 static u64 calc_task_load_actual(struct task_ctx *taskc)
-- 
2.45.2


From 827187d213b0762c7d321f78eb51570eff46c88d Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Sat, 20 Jul 2024 17:37:27 +0900
Subject: [PATCH 19/20] scx_lavd: adjust ineligible duration according to
 task's lat_cri

Further depenalize above-average latency-critical tasks and penalize
further below-avergage latency-critical tasks in ineligibility duration.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/intf.h     | 17 ++++++------
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 36 +++++++++++++++++++++----
 2 files changed, 40 insertions(+), 13 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/intf.h b/scheds/rust/scx_lavd/src/bpf/intf.h
index e50209f..067d0bf 100644
--- a/scheds/rust/scx_lavd/src/bpf/intf.h
+++ b/scheds/rust/scx_lavd/src/bpf/intf.h
@@ -56,9 +56,9 @@ enum consts {
 	LAVD_TIME_INFINITY_NS		= SCX_SLICE_INF,
 	LAVD_MAX_RETRY			= 4,
 
-	LAVD_TARGETED_LATENCY_NS	= (15 * NSEC_PER_MSEC),
-	LAVD_SLICE_MIN_NS		= (30 * NSEC_PER_USEC), /* min time slice */
-	LAVD_SLICE_MAX_NS		= ( 3 * NSEC_PER_MSEC), /* max time slice */
+	LAVD_TARGETED_LATENCY_NS	= (15ULL * NSEC_PER_MSEC),
+	LAVD_SLICE_MIN_NS		= (30ULL * NSEC_PER_USEC), /* min time slice */
+	LAVD_SLICE_MAX_NS		= ( 3ULL * NSEC_PER_MSEC), /* max time slice */
 	LAVD_SLICE_UNDECIDED		= SCX_SLICE_INF,
 	LAVD_VDL_LOOSENESS_FT		= 100,
 
@@ -69,10 +69,11 @@ enum consts {
 
 	LAVD_SLICE_BOOST_MAX_FT		= 2, /* maximum additional 2x of slice */
 	LAVD_SLICE_BOOST_MAX_STEP	= 8, /* 8 slice exhausitions in a row */
+	LAVD_GREEDY_RATIO_NEW		= 2000,
 	LAVD_GREEDY_RATIO_MAX		= USHRT_MAX,
 
 	LAVD_ELIGIBLE_TIME_LAT_FT	= 16,
-	LAVD_ELIGIBLE_TIME_MAX		= (10 * LAVD_TARGETED_LATENCY_NS),
+	LAVD_ELIGIBLE_TIME_MAX		= (1ULL * LAVD_TIME_ONE_SEC),
 	LAVD_REFILL_NR			= 2,
 
 	LAVD_CPU_UTIL_MAX		= 1000, /* 100.0% */
@@ -81,14 +82,14 @@ enum consts {
 	LAVD_CPU_ID_NONE		= ((u32)-1),
 	LAVD_CPU_ID_MAX			= 512,
 
-	LAVD_PREEMPT_KICK_MARGIN	= (2 * NSEC_PER_USEC),
-	LAVD_PREEMPT_TICK_MARGIN	= (1 * NSEC_PER_USEC),
+	LAVD_PREEMPT_KICK_MARGIN	= (2ULL * NSEC_PER_USEC),
+	LAVD_PREEMPT_TICK_MARGIN	= (1ULL * NSEC_PER_USEC),
 
-	LAVD_SYS_STAT_INTERVAL_NS	= (25 * NSEC_PER_MSEC),
+	LAVD_SYS_STAT_INTERVAL_NS	= (25ULL * NSEC_PER_MSEC),
 	LAVD_TC_PER_CORE_MAX_CTUIL	= 500, /* maximum per-core CPU utilization */
 	LAVD_TC_NR_ACTIVE_MIN		= 1, /* num of mininum active cores */
 	LAVD_TC_NR_OVRFLW		= 1, /* num of overflow cores */
-	LAVD_TC_CPU_PIN_INTERVAL	= (100 * NSEC_PER_MSEC),
+	LAVD_TC_CPU_PIN_INTERVAL	= (100ULL * NSEC_PER_MSEC),
 	LAVD_TC_CPU_PIN_INTERVAL_DIV	= (LAVD_TC_CPU_PIN_INTERVAL /
 					   LAVD_SYS_STAT_INTERVAL_NS),
 
diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index c13e71f..c97b99c 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -925,6 +925,11 @@ static u32 calc_greedy_ratio(struct task_struct *p, struct task_ctx *taskc)
 	struct sys_stat *stat_cur = get_sys_stat_cur();
 	u32 ratio;
 
+	if (!have_scheduled(taskc)) {
+		ratio = LAVD_GREEDY_RATIO_NEW;
+		goto out;
+	}
+
 	/*
 	 * The greedy ratio of a task represents how much time the task
 	 * overspent CPU time compared to the ideal, fair CPU allocation. It is
@@ -932,6 +937,8 @@ static u32 calc_greedy_ratio(struct task_struct *p, struct task_ctx *taskc)
 	 * system.
 	 */
 	ratio = (1000 * taskc->svc_time) / stat_cur->avg_svc_time;
+
+out:
 	taskc->greedy_ratio = ratio;
 	return ratio;
 }
@@ -993,7 +1000,8 @@ static u64 calc_eligible_delta(struct task_struct *p, struct task_ctx *taskc)
 	 *	task's interval_old * greedy_ratio =
 	 *	when the task become eligible.
 	 */
-	u64 delta_ns;
+	struct sys_stat *stat_cur = get_sys_stat_cur();
+	u64 delta_ns, lat_cri_ft;
 
 	/*
 	 * Get how greedy this task has been to enforce fairness if necessary.
@@ -1010,8 +1018,28 @@ static u64 calc_eligible_delta(struct task_struct *p, struct task_ctx *taskc)
 		goto out;
 	}
 
-	delta_ns = (LAVD_TIME_ONE_SEC / (1000 * taskc->run_freq)) *
-		   (taskc->greedy_ratio);
+
+	/*
+	 * Calculate ineligible duration based on greedy ratio, run_freq, and
+	 * lat_cri.
+	 */
+	delta_ns = (LAVD_TIME_ONE_SEC / (1000 * (taskc->run_freq + 1))) *
+		   taskc->greedy_ratio;
+
+	if (stat_cur->avg_lat_cri < taskc->lat_cri) {
+		/*
+		 * Prioritize above-average latency-critical tasks.
+		 */
+		lat_cri_ft = taskc->lat_cri - stat_cur->avg_lat_cri + 1;
+		delta_ns /= lat_cri_ft;
+	}
+	else {
+		/*
+		 * Deprioritize below-average latency-critical tasks.
+		 */
+		lat_cri_ft = stat_cur->avg_lat_cri - taskc->lat_cri + 1;
+		delta_ns *= lat_cri_ft;
+	}
 
 	if (delta_ns > LAVD_ELIGIBLE_TIME_MAX)
 		delta_ns = LAVD_ELIGIBLE_TIME_MAX;
@@ -2564,8 +2592,6 @@ static void init_task_ctx(struct task_struct *p, struct task_ctx *taskc)
 	taskc->last_running_clk = now; /* for run_time_ns */
 	taskc->last_stopping_clk = now; /* for run_time_ns */
 	taskc->run_time_ns = LAVD_SLICE_MAX_NS;
-	taskc->run_freq = 0;
-	taskc->greedy_ratio = 1000;
 	taskc->victim_cpu = (s32)LAVD_CPU_ID_NONE;
 }
 
-- 
2.45.2


From add96f0e189ea65c422a7a04c3ec46534464e332 Mon Sep 17 00:00:00 2001
From: Changwoo Min <changwoo@igalia.com>
Date: Sat, 20 Jul 2024 17:49:12 +0900
Subject: [PATCH 20/20] scx_lavd: do not maintain ineligible runnable tasks
 separately

With all the other optimizations and tunings, it turns out that maintaining
two runqueues has more harm than good.

Signed-off-by: Changwoo Min <changwoo@igalia.com>
---
 scheds/rust/scx_lavd/src/bpf/main.bpf.c | 167 ++----------------------
 1 file changed, 9 insertions(+), 158 deletions(-)

diff --git a/scheds/rust/scx_lavd/src/bpf/main.bpf.c b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
index c97b99c..6601766 100644
--- a/scheds/rust/scx_lavd/src/bpf/main.bpf.c
+++ b/scheds/rust/scx_lavd/src/bpf/main.bpf.c
@@ -208,20 +208,6 @@ static volatile int	__sys_stat_idx;
 private(LAVD) struct bpf_cpumask __kptr *active_cpumask; /* CPU mask for active CPUs */
 private(LAVD) struct bpf_cpumask __kptr *ovrflw_cpumask; /* CPU mask for overflow CPUs */
 
-/*
- * Ineligible runnable queue
- */
-struct task_node {
-	u64			vdeadline;	/* key */
-	u64			pid;		/* value */
-	struct bpf_rb_node	node;
-};
-
-private(LAVD) struct bpf_spin_lock	ineli_lock;
-private(LAVD) struct bpf_rb_root	ineli_rq __contains(task_node, node);
-static u64				nr_ineli;
-static u64				refill_clk; /* when an eliglble DSQ is refilled */
-
 /*
  * CPU topology
  */
@@ -1152,11 +1138,6 @@ static u64 clamp_time_slice_ns(u64 slice)
 	return slice;
 }
 
-static u64 rq_nr_queued(void)
-{
-	return scx_bpf_dsq_nr_queued(LAVD_ELIGIBLE_DSQ) + READ_ONCE(nr_ineli);
-}
-
 static u64 calc_time_slice(struct task_struct *p, struct task_ctx *taskc)
 {
 	struct sys_stat *stat_cur = get_sys_stat_cur();
@@ -1166,7 +1147,7 @@ static u64 calc_time_slice(struct task_struct *p, struct task_ctx *taskc)
 	 * The time slice should be short enough to schedule all runnable tasks
 	 * at least once within a targeted latency.
 	 */
-	nr_queued = rq_nr_queued() + 1;
+	nr_queued = scx_bpf_dsq_nr_queued(LAVD_ELIGIBLE_DSQ) + 1;
 	slice = (LAVD_TARGETED_LATENCY_NS * stat_cur->nr_active) / nr_queued;
 	if (is_eligible(taskc)) {
 		slice += (LAVD_SLICE_BOOST_MAX_FT * slice *
@@ -1249,7 +1230,7 @@ static void advance_cur_logical_clk(struct task_ctx *taskc)
 	 * Advance the clock up to the task's deadline. When overloaded,
 	 * advnace the clock slower so other can jump in the run queue.
 	 */
-	nr_queued = max(rq_nr_queued(), 1);
+	nr_queued = max(scx_bpf_dsq_nr_queued(LAVD_ELIGIBLE_DSQ), 1);
 	delta = (vlc - clc) / nr_queued;
 	new_clk = clc + delta;
 
@@ -1721,23 +1702,11 @@ static bool try_yield_current_cpu(struct task_struct *p_run,
 	return ret;
 }
 
-static bool less(struct bpf_rb_node *a, const struct bpf_rb_node *b)
-{
-	struct task_node *node_a;
-	struct task_node *node_b;
-
-	node_a = container_of(a, struct task_node, node);
-	node_b = container_of(b, struct task_node, node);
-
-	return node_a->vdeadline < node_b->vdeadline;
-}
-
 static void put_global_rq(struct task_struct *p, struct task_ctx *taskc,
 			  struct cpu_ctx *cpuc, u64 enq_flags)
 {
 	struct task_ctx *taskc_run;
 	struct task_struct *p_run;
-	struct task_node *t;
 
 	/*
 	 * Calculate when a tack can be scheduled.
@@ -1764,30 +1733,11 @@ static void put_global_rq(struct task_struct *p, struct task_ctx *taskc,
 		taskc_run = try_get_task_ctx(p_run);
 		if (taskc_run && p_run->scx.slice != 0)
 			try_yield_current_cpu(p_run, cpuc, taskc_run);
-
-		goto dispatch_out;
 	}
 
-	/*
-	 * If the task is ineligible, keep it to the ineligible run queue.
-	 */
-	t = bpf_obj_new(typeof(*t));
-	if (!t)
-		goto dispatch_out;
-
-	t->vdeadline = taskc->vdeadline_log_clk;
-	t->pid = p->pid;
-
-	bpf_spin_lock(&ineli_lock);
-	bpf_rbtree_add(&ineli_rq, &t->node, less);
-	WRITE_ONCE(nr_ineli, nr_ineli + 1);
-	bpf_spin_unlock(&ineli_lock);
-	return;
-
 	/*
 	 * Enqueue the task to the eligible DSQ based on its virtual deadline.
 	 */
-dispatch_out:
 	scx_bpf_dispatch_vtime(p, LAVD_ELIGIBLE_DSQ, LAVD_SLICE_UNDECIDED,
 			       taskc->vdeadline_log_clk, enq_flags);
 	return;
@@ -1963,101 +1913,6 @@ static bool use_full_cpus(void)
 	       ((stat_cur->nr_active + LAVD_TC_NR_OVRFLW) >= nr_cpus_onln);
 }
 
-static bool refill_eligible_dsq(s32 cpu)
-{
-	struct task_struct *p;
-	struct task_ctx *taskc;
-	struct cpu_ctx *cpuc;
-	struct bpf_rb_node *node;
-	struct task_node *t;
-	u64 pid;
-
-	/*
-	 * Fetch the first task on the ineligible rq.
-	 */
-	bpf_spin_lock(&ineli_lock);
-	node = bpf_rbtree_first(&ineli_rq);
-	if (!node) {
-		bpf_spin_unlock(&ineli_lock);
-		return false;
-	}
-	t = container_of(node, struct task_node, node);
-	pid = t->pid;
-
-	node = bpf_rbtree_remove(&ineli_rq, &t->node);
-	WRITE_ONCE(nr_ineli, nr_ineli - 1);
-	bpf_spin_unlock(&ineli_lock);
-
-	if (node) {
-		t = container_of(node, struct task_node, node);
-		bpf_obj_drop(t);
-	}
-
-	/*
-	 * Recalculate when to run for the task.
-	 */
-	p = bpf_task_from_pid(pid);
-	if (!p)
-		return false;
-	taskc = get_task_ctx(p);
-	cpuc = get_cpu_ctx_id(cpu);
-	if (!cpuc || !taskc) {
-		bpf_task_release(p);
-		return false;
-	}
-
-	calc_when_to_run(p, taskc, cpuc, 0);
-
-	/*
-	 * Dispatch the first task to the eligible queue for future
-	 * scheduling.
-	 */
-	scx_bpf_dispatch_vtime(p, LAVD_ELIGIBLE_DSQ,
-			       LAVD_SLICE_UNDECIDED,
-			       taskc->vdeadline_log_clk, 0);
-	bpf_task_release(p);
-
-	/*
-	 * Update the refill clock.
-	 */
-	WRITE_ONCE(refill_clk, bpf_ktime_get_ns());
-
-	return true;
-}
-
-static bool is_time_to_refill(void)
-{
-	return (READ_ONCE(refill_clk) + LAVD_TARGETED_LATENCY_NS) <
-		bpf_ktime_get_ns();
-}
-
-static bool consume_task(s32 cpu)
-{
-	bool ret;
-
-	/*
-	 * If the ineligible rq has not consumed too long,
-	 * move a task from the ineligible rq to the eligible dsq.
-	 */
-	if (is_time_to_refill())
-		refill_eligible_dsq(cpu);
-
-	/*
-	 * Consume a task from the eliglble dsq.
-	 */
-	ret = scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
-	if (!ret) {
-		/*
-		 * If there is not task to run on the eligible dsq, refill it
-		 * then try it again.
-		 */
-		if (refill_eligible_dsq(cpu))
-			ret = scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
-	}
-
-	return ret;
-}
-
 void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 {
 	struct bpf_cpumask *active, *ovrflw;
@@ -2067,7 +1922,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 	 * If all CPUs are using, directly consume without checking CPU masks.
 	 */
 	if (use_full_cpus()) {
-		consume_task(cpu);
+		scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
 		return;
 	}
 
@@ -2088,7 +1943,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 	 */
 	if (bpf_cpumask_test_cpu(cpu, cast_mask(active)) ||
 	    bpf_cpumask_test_cpu(cpu, cast_mask(ovrflw))) {
-		consume_task(cpu);
+		scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
 		goto unlock_out;
 	}
 
@@ -2103,7 +1958,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 		 * kworker, etc).
 		 */
 		if (is_kernel_task(p)) {
-			consume_task(cpu);
+			scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
 			break;
 		}
 
@@ -2135,7 +1990,7 @@ void BPF_STRUCT_OPS(lavd_dispatch, s32 cpu, struct task_struct *prev)
 		 * cores. We will optimize this path after introducing per-core
 		 * DSQ.
 		 */
-		consume_task(cpu);
+		scx_bpf_consume(LAVD_ELIGIBLE_DSQ);
 
 		/*
 		 * This is the first time a particular pinned user-space task
@@ -2637,7 +2492,7 @@ s32 BPF_STRUCT_OPS(lavd_init_task, struct task_struct *p,
 	return 0;
 }
 
-static s32 init_dsqs(void)
+static s32 init_dsq(void)
 {
 	int err;
 
@@ -2647,10 +2502,6 @@ static s32 init_dsqs(void)
 		return err;
 	}
 
-	/*
-	 * Nothing to init for ineli_rq.
-	 */
-
 	return 0;
 }
 
@@ -2757,9 +2608,9 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(lavd_init)
 	int err;
 
 	/*
-	 * Create central task queues.
+	 * Create a central task queue.
 	 */
-	err = init_dsqs();
+	err = init_dsq();
 	if (err)
 		return err;
 
-- 
2.45.2

